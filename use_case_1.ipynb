{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# üéõÔ∏èProgram Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:59:13.780351Z",
     "start_time": "2023-09-11T11:59:13.515559Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Standard Python Imports\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Third-Party Imports (requires pip install)\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pymongo import MongoClient\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Hermes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Load Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:39:24.805829Z",
     "start_time": "2023-09-11T11:39:24.802247Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the Jira Data Sources JSON\n",
    "with open('./data/jira_issuetype_thematic_analysis.json') as f:\n",
    "    jira_issuetypes = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Define Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T12:10:26.904212Z",
     "start_time": "2023-09-11T12:10:26.652103Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the spacy English model. Must first run \"python -m spacy download en\" in terminal to download the package\n",
    "nlp_spacy_en = spacy.load('en_core_web_sm')\n",
    "# Use Case 1 Globals\n",
    "data_folder = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Get Use-Case Data: 1Ô∏è‚É£ Use Database or 2Ô∏è‚É£ Attached TSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### 1Ô∏è‚É£ Export from DB and into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T12:10:31.571096Z",
     "start_time": "2023-09-11T12:10:31.566422Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Connect to the Mongo database\n",
    "mongo_client = MongoClient()\n",
    "db = mongo_client['JiraRepos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:39:54.342655Z",
     "start_time": "2023-09-11T11:39:28.300908Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###############################################################| 12/12 [00:53<00:00,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1,538 (Apache, Story)\n",
      " 3,942 (Hyperledger, Story)\n",
      " 4,536 (IntelDAOS, Story)\n",
      "     0 (JFrog, Story)\n",
      " 2,788 (JiraEcosystem, Story)\n",
      "   245 (MongoDB, Story)\n",
      " 2,375 (Qt, User Story)\n",
      "13,452 (RedHat, Story)\n",
      "   337 (RedHat, Requirement)\n",
      "   220 (Sonatype, Story)\n",
      " 2,286 (Spring, Story)\n",
      "   301 (SecondLife, Story)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gather list of Jiras and their \"Story\" issue type\n",
    "jira_story_map = []\n",
    "for jira, issuetypes in jira_issuetypes.items():\n",
    "    for issuetype, issuetype_obj in issuetypes.items():\n",
    "        if issuetype_obj['theme'] == 'Requirements' and issuetype_obj['code'] == 'Story':\n",
    "            jira_story_map.append((jira, issuetype))\n",
    "\n",
    "# Retrieve counts of the \"Story\" issue type per Jira\n",
    "results = []\n",
    "for jira, story_issuetype in tqdm(jira_story_map, total=len(jira_story_map), ncols=100, ascii=True):\n",
    "    result = list(db[jira].aggregate([\n",
    "        # Match only certain issues\n",
    "        { '$match': { '$and': [\n",
    "            # Limit the data to resolved issues, otherwise we have \"in progress\" issues which are not yet complete\n",
    "            { 'fields.resolution': { '$ne': None } },\n",
    "            # Match the Jira-specific name for their Story issue type\n",
    "            { 'fields.issuetype.name': story_issuetype },\n",
    "        ]}},\n",
    "        # Count the number of documents at this point in the pipeline\n",
    "        { '$count': 'count' }\n",
    "    ], allowDiskUse=True))\n",
    "    issuetype_num_issues = int(result[0]['count']) if len(result) and 'count' in result[0] else 0\n",
    "    # Save this result to be printed later\n",
    "    results.append((jira, story_issuetype, issuetype_num_issues))\n",
    "\n",
    "for jira, story_issuetype, issuetype_num_issues in results:\n",
    "    print(f\"{issuetype_num_issues: >6,} ({jira}, {story_issuetype})\")\n",
    "    \n",
    "# RESULT (for those without the database data)\n",
    "#  1,538 (Apache, Story)\n",
    "#  3,942 (Hyperledger, Story)\n",
    "#  4,536 (IntelDAOS, Story)\n",
    "#      0 (JFrog, Story)\n",
    "#  2,788 (JiraEcosystem, Story)\n",
    "#    245 (MongoDB, Story)\n",
    "#  2,375 (Qt, User Story)\n",
    "# 13,452 (RedHat, Story)\n",
    "#    337 (RedHat, Requirement)\n",
    "#    220 (Sonatype, Story)\n",
    "#  2,286 (Spring, Story)\n",
    "#    301 (SecondLife, Story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:39:54.499434Z",
     "start_time": "2023-09-11T11:39:54.341050Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Select the desired Jira repo and issue type\n",
    "chosen_jira = 'RedHat'\n",
    "chosen_issuetype = 'Story'\n",
    "\n",
    "# Get the issues for the chosen Jira\n",
    "issues = list(db[chosen_jira].aggregate([\n",
    "    # Match only certain issues\n",
    "    { '$match': { '$and': [\n",
    "        # Limit the data to resolved issues, otherwise we have \"in progress\" issues which are not yet complete\n",
    "        { 'fields.resolution': { '$ne': None } },\n",
    "        # Match the Jira-specific name for their Story issue type\n",
    "        { 'fields.issuetype.name': chosen_issuetype },\n",
    "        # Only get documents where the description is not empty\n",
    "        { 'fields.description': { '$ne': None } },\n",
    "    ]}},\n",
    "    # Retrieve and rename fields \n",
    "    { '$project': {\n",
    "        '_id': 0, 'id': 1,\n",
    "        'description': '$fields.description',\n",
    "    }}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:39:54.519322Z",
     "start_time": "2023-09-11T11:39:54.500050Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Write these issues to CSV so they can be easily loaded later without the full database.\n",
    "with open(f\"{data_folder}/use_case_1_data.tsv\", 'w', newline='') as tsv_file:\n",
    "    writer = csv.writer(tsv_file, delimiter='\\t', quotechar='\"', lineterminator='\\n')\n",
    "    writer.writerow(issues[0].keys())\n",
    "    writer.writerows([issue.values() for issue in issues])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### 2Ô∏è‚É£ Load TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:39:54.539676Z",
     "start_time": "2023-09-11T11:39:54.537880Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Increase the limit of the csv data we can read in\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "with open(f\"{data_folder}/use_case_1_data.tsv\") as tsv_file:\n",
    "    reader = csv.reader(tsv_file, delimiter='\\t', quotechar='\"')\n",
    "    headers = next(reader)\n",
    "    issues = [dict(zip(headers, row)) for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T12:16:02.171753Z",
     "start_time": "2023-09-11T12:16:02.160681Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11,911 issues.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(issues):,} issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Issues: 1,601\n"
     ]
    }
   ],
   "source": [
    "# Remove all issues that don't have at least \"As a\" in the description, which is a minimum for a user story\n",
    "issues = [issue for issue in issues if 'As a' in issue['description']]\n",
    "print(f\"Remaining Issues: {len(issues):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:39:54.545581Z",
     "start_time": "2023-09-11T11:39:54.542411Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------- RANDOM ISSUE --------------------------------------------------\n",
      "\n",
      "Issue ID: 13269613\n",
      "\n",
      "h1. Story\n",
      "\n",
      "As an administrator of cluster logging, I want the operator to raise a kubernete event if the top ClusterLogForwarder Status has conditions Invalid=true or Degraded=true.\n",
      "h1. Acceptance Criteria\n",
      " * Operator raises event when the CLF reconciles as invalid or degraded.\n",
      " * Event contains human readable summary and status information (review format of existing k8s events)\n",
      " * Once an Event is raised, no further events are raised unless/until status changes (e.g. degraded for a new reason)\n",
      " * The CLO must-gather is updated to capture events\n",
      " * Document created events\n"
     ]
    }
   ],
   "source": [
    "# Get random samples of issues for display\n",
    "number_of_sample_issues = 1\n",
    "for issue in random.sample(issues, number_of_sample_issues):\n",
    "    print(f\"\\n{'-'*50} RANDOM ISSUE {'-'*50}\\n\")\n",
    "    print(f\"Issue ID: {issue['id']}\\n\")\n",
    "    print(issue['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### NLP Techniques for Detecting Ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T12:10:50.115083Z",
     "start_time": "2023-09-11T12:10:50.112118Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def check_lexical(issue_id, text, ambiguity_type_obj, ambs_found):\n",
    "\n",
    "    def whole_phrase_regexp(phrase):\n",
    "        # Handle spaces in strings\n",
    "        phrase = phrase.replace(' ', r'\\s')\n",
    "        try:\n",
    "            return re.compile(r'\\b{0}\\b'.format(phrase), flags=re.I|re.X)\n",
    "        except re.error:\n",
    "            return re.compile(r'\\b\\{0}\\b'.format(phrase), flags=re.I|re.X)\n",
    "\n",
    "    # Go over all phrases in lexicon\n",
    "    for word_phrase in ambiguity_type_obj['lexicon']:\n",
    "        # Search for all word phrases in sentence\n",
    "        for match in re.finditer(whole_phrase_regexp(word_phrase), text):\n",
    "            ambs_found[issue_id].append({\n",
    "                'text': match[0],\n",
    "                'amb_type': ambiguity_type_obj['title'],\n",
    "                'index_start': match.start(),\n",
    "                'index_end': match.end()\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T12:10:50.468953Z",
     "start_time": "2023-09-11T12:10:50.464785Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def check_regexes(issue_id, text, ambiguity_type_obj, ambs_found):\n",
    "    # Create Python regular expression object\n",
    "    regexp = re.compile(ambiguity_type_obj['regexp'], flags=re.I|re.X)\n",
    "    # Search for all regexps in requirement\n",
    "    for match in re.finditer(regexp, text):\n",
    "        ambs_found[issue_id].append({\n",
    "            'text': match[0],\n",
    "            'amb_type': ambiguity_type_obj['title'],\n",
    "            'index_start': match.start(),\n",
    "            'index_end': match.end()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T12:10:50.925489Z",
     "start_time": "2023-09-11T12:10:50.917977Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def check_pos_regexes(issue_id, text, ambiguity_type_obj, ambs_found):\n",
    "    # Get the original indexes, before the truple design messed with it\n",
    "    def get_original_indexes(_req_original_string, _req_tokenized_string, _req_truple_string, _match):\n",
    "        # Add up extra letters (indexes) due to truple design\n",
    "        def count_extra_indexes(up_to_index):\n",
    "            # Count the extra letters in a given truple\n",
    "            def count_extra_letters(req_truple):\n",
    "                try:\n",
    "                    split = req_truple.split('¬∞')\n",
    "                    return len(split[1]) + len(split[2]) + 2\n",
    "                except:\n",
    "                    return 0\n",
    "\n",
    "            # Calculate space added by tokenization process\n",
    "            def count_tokenize_space(_req_original_string, _req_tokenized_string):\n",
    "                orig_i = 0\n",
    "                tokn_i = 0\n",
    "                while tokn_i < len(_req_tokenized_string):\n",
    "                    if _req_original_string[orig_i] != _req_tokenized_string[tokn_i]:\n",
    "                        tokn_i += 1\n",
    "                        continue\n",
    "                    orig_i += 1\n",
    "                    tokn_i += 1\n",
    "                return tokn_i - orig_i\n",
    "\n",
    "            # Remove string after the index\n",
    "            words_pre_index = _req_truple_string[:up_to_index].split()\n",
    "            # Calculate extra indexes added by the truple system\n",
    "            extra_truple_indexes = sum([count_extra_letters(req_truple) for req_truple in words_pre_index])\n",
    "            # Update the 'up_to_index' to reflect the newly discovered mistakes\n",
    "            up_to_index = up_to_index - extra_truple_indexes\n",
    "            # Calculate the extra indexes added by the tokenizing process\n",
    "            extra_tokenize_space = count_tokenize_space(_req_original_string[:up_to_index], _req_tokenized_string[:up_to_index])\n",
    "\n",
    "            return extra_truple_indexes + extra_tokenize_space\n",
    "        return (\n",
    "            _match.start() - count_extra_indexes(_match.start()),\n",
    "            _match.end() - count_extra_indexes(_match.end()))\n",
    "\n",
    "    doc = nlp_spacy_en(text)\n",
    "\n",
    "    # Create list of truples strings (word, POS tag, lemma) with degree symbol in between each part\n",
    "    truple_list = ['{0}¬∞{1}¬∞{2}'.format(token.text, token.tag_, token.lemma_) for token in doc]\n",
    "\n",
    "    # Create variables for easier and more readable use later\n",
    "    req_original_string = text\n",
    "    req_tokenized_string = ' '.join([token.text for token in doc])\n",
    "    req_truple_string = ' '.join(truple_list)  # Convert into string so regex can be performed\n",
    "\n",
    "    # Create Python regular expression object\n",
    "    regexp = re.compile(ambiguity_type_obj['regexp'], flags=re.I|re.X)\n",
    "    # Search for all regexps in requirement\n",
    "    for match in re.finditer(regexp, req_truple_string):\n",
    "        # Get the original indexes, since the truple string design messes with them\n",
    "        orig_indexes = get_original_indexes(\n",
    "            req_original_string, req_tokenized_string, req_truple_string, match)\n",
    "\n",
    "        orig_text = ' '.join([req_truple.split('¬∞')[0] for req_truple in match[0].split()])\n",
    "        # Save this found ambiguity\n",
    "        ambs_found[issue_id].append({\n",
    "            'text': match[0],\n",
    "            'amb_type': ambiguity_type_obj['title'],\n",
    "            'index_start': orig_indexes[0],\n",
    "            'index_end': orig_indexes[1],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_compound_nouns(issue_id, text, ambiguity_type_obj, ambs_found):\n",
    "\n",
    "    # Initialise a spacy doc object for advanced NLP applications\n",
    "    doc = nlp_spacy_en(text)\n",
    "\n",
    "    # Search each noun chunk for consecutive nouns \n",
    "    for chunk in doc.noun_chunks:\n",
    "        compound_list = [token for token in chunk if (token.dep_ == 'compound')\n",
    "                            or (token.tag_ in ('NN', 'NNS', 'NNP', 'NNPS') and token.dep_ in ('nmod', 'amod'))\n",
    "                            or (token.tag_ == 'VBG' and token.dep_ == 'nmod')\n",
    "                            or token == chunk.root]\n",
    "        if len(compound_list) > 2:\n",
    "            new_indexes = [compound_list[0].idx, compound_list[-1].idx + len(compound_list[-1].text)]\n",
    "            original_text = text[new_indexes[0]:new_indexes[1]]\n",
    "            # Save this found ambiguity\n",
    "            ambs_found[issue_id].append({\n",
    "                'text': original_text,\n",
    "                'amb_type': ambiguity_type_obj['title'],\n",
    "                'index_start': new_indexes[0],\n",
    "                'index_end': new_indexes[1],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nominalisations(issue_id, text, ambiguity_type_obj, ambs_found):\n",
    "\n",
    "    # Initialise a spacy doc object for advanced NLP applications\n",
    "    doc = nlp_spacy_en(text)\n",
    "\n",
    "    # Generate a list of gerund nouminalizations that have pos VB\n",
    "    nominalizations = [[t for t in token.subtree] for token in doc\n",
    "                    if(token.text[-3:] in ambiguity_type_obj['gerund']\n",
    "                        or token.text[-4:] in ambiguity_type_obj['gerund_plural'])\n",
    "                        and token.tag_ == 'VBG'\n",
    "                        and token.dep_ not in ('root', 'aux', 'advmod', 'compound', 'acl')\n",
    "                        and doc[token.i - 1].dep_ != 'aux'\n",
    "                        and token.text.lower() not in ambiguity_type_obj['rule_exceptions']]\n",
    "\n",
    "    # Generate a list of nominalizations with pos NN based on suffixes\n",
    "    nouns = [token for token in doc if (token.lemma_[-4:] in ambiguity_type_obj['suffixes_len4']\n",
    "                                        or token.lemma_[-3:] in ambiguity_type_obj['suffixes_len3']\n",
    "                                        or token.lemma_[-2:] in ambiguity_type_obj['suffixes_len2'])\n",
    "            and token.tag_ in ('NN', 'NNS')\n",
    "            and wn.synsets(token.text)]\n",
    "    # Filter list of nouns based on semantic hierarchy\n",
    "    for token in nouns:\n",
    "        # Generate and flatten the list of hypernyms for each noun\n",
    "        hypernyms = list(\n",
    "            map(lambda x: x.name().split('.')[0],\n",
    "                sum(wn.synsets(token.text)[0].hypernym_paths(), [])))\n",
    "        # Only consider nouns that express an event or a process\n",
    "        if [l for l in hypernyms if l in ['event', 'process', 'act']] \\\n",
    "                and token.text.lower() not in ambiguity_type_obj['rule_exceptions']:\n",
    "            nominalizations.append([t for t in token.subtree])\n",
    "\n",
    "    # Return all ambiguous nominalization sequences found\n",
    "    for token_seq in nominalizations:\n",
    "        if token_seq:\n",
    "            new_text = ' '.join([t.text for t in token_seq])\n",
    "            new_indexes = [token_seq[0].idx, token_seq[-1].idx + len(token_seq[-1].text)]\n",
    "            # Save this found ambiguity\n",
    "            ambs_found[issue_id].append({\n",
    "                'text': new_text,\n",
    "                'amb_type': ambiguity_type_obj['title'],\n",
    "                'index_start': new_indexes[0],\n",
    "                'index_end': new_indexes[1],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T12:10:51.630890Z",
     "start_time": "2023-09-11T12:10:51.621413Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nlp_method_map = {\n",
    "    'check_lexical': check_lexical,\n",
    "    'check_regexes': check_regexes,\n",
    "    'check_pos_regexes': check_pos_regexes,\n",
    "    'check_compound_nouns': check_compound_nouns,\n",
    "    'check_nominalisations': check_nominalisations,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_ambiguities(issues, lexicons, sample_data_n=None):\n",
    "\n",
    "    # For time reasons, it is helpful to check just a sample of the issues\n",
    "    issues_to_check = issues\n",
    "    if sample_data_n and sample_data_n < len(issues):\n",
    "        issues_to_check = random.sample(issues_to_check, sample_data_n)\n",
    "\n",
    "    # Store all ambiguities\n",
    "    issues_ambs_found = defaultdict(list)\n",
    "\n",
    "    # Go through each issue description, and check for ambiguities\n",
    "    for issue in tqdm(issues_to_check, total=len(issues_to_check), ncols=100, ascii=True):\n",
    "        # Iterate through all lexicons, where each one represents a different form of NLP algorithms\n",
    "        for nlp_check, lexicon in lexicons.items():\n",
    "            # Get the NLP method as needed by the lexicon\n",
    "            lexicon_nlp_method = nlp_method_map[nlp_check]\n",
    "            # For each lexicon, iterate through the available ambiguity checks that can be performed\n",
    "            for ambiguity_type_obj in lexicon.values():\n",
    "                # Apply the NLP method, which will add any found ambiguities to the \"ambs_found\" list\n",
    "                lexicon_nlp_method(issue['id'], issue['description'], ambiguity_type_obj, issues_ambs_found)\n",
    "        \n",
    "    # Return all ambiguits found\n",
    "    return issues_ambs_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_issue_ambiguities(issues_ambs_found, sample_data_n=None):\n",
    "\n",
    "    # If there are no ambiguities, this provides a helpful message instead of a blank output\n",
    "    if not issues_ambs_found:\n",
    "        print('There are no ambiguities to display')\n",
    "        return\n",
    "\n",
    "    # For time reasons, it may be better to just display a sample of issues\n",
    "    if sample_data_n and sample_data_n < len(issues_ambs_found):\n",
    "        # Get a random sample of keys\n",
    "        issues_ambs_found_keys = random.sample(list(issues_ambs_found.keys()), sample_data_n)\n",
    "        # Use those keys to build a sample of the original dict\n",
    "        issues_ambs_found = {key: issues_ambs_found[key] for key in issues_ambs_found_keys}\n",
    "\n",
    "    # Display each group of ambiguities, one issue at a time\n",
    "    for amb_issue_id, amb_found in issues_ambs_found.items():\n",
    "        # Print the issue description\n",
    "        amb_issue = [issue for issue in issues if issue['id'] == amb_issue_id][0]\n",
    "        print(f\"\\n{'-'*50} Issue ID: {amb_issue['id']} {'-'*50}\\n\")\n",
    "        print(f\"{amb_issue['description']}\\n\")\n",
    "        # Print the ambs found\n",
    "        print('Ambiguities Found:')\n",
    "        pprint(amb_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Ambiguit Detection: Subjective Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T12:10:53.145107Z",
     "start_time": "2023-09-11T12:10:53.143058Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Some example Subjective Language detection lexicons\n",
    "subject_language_lexicons = {\n",
    "    'check_lexical': {\n",
    "        'dangerous_plural' : {\n",
    "            'title'             : 'Dangerous Plural',\n",
    "            'description'       : 'Potentially dangerous plural.',\n",
    "            'lexicon'           : ['all', 'each', 'every', 'any', 'few', 'little', 'many', 'much', 'several', 'some', 'a lot'],\n",
    "            'language_construct': 'Subjective Language',\n",
    "            'lit_reference'     : 'Tjong SF, Berry DM. The design of SREE‚Äîa prototype potential ambiguity finder for requirements specifications and lessons learned. InInternational Working Conference on Requirements Engineering: Foundation for Software Quality 2013 Apr 8 (pp. 80-95). Berlin, Heidelberg: Springer Berlin Heidelberg.',\n",
    "        },\n",
    "        'inside_behaviour' : {\n",
    "            'title'             : 'Inside Behaviour',\n",
    "            'description'       : 'These expressions do not specify the \"outside boundaries\" behaviour.',\n",
    "            'lexicon'           : ['until', 'during', 'through', 'after', 'at'],\n",
    "            'language_construct': 'Subjective Language',\n",
    "            'lit_reference'     : 'Tjong SF, Berry DM. The design of SREE‚Äîa prototype potential ambiguity finder for requirements specifications and lessons learned. InInternational Working Conference on Requirements Engineering: Foundation for Software Quality 2013 Apr 8 (pp. 80-95). Berlin, Heidelberg: Springer Berlin Heidelberg.',\n",
    "        },\n",
    "    },\n",
    "    'check_regexes': {\n",
    "        'unclear_inclusion' : {\n",
    "            'title'             : 'Unclear Inclusion',\n",
    "            'description'       : 'Up to with unclear inclusion.',\n",
    "            'regexp'            : 'up\\\\sto\\\\s(?!.*including|excluding)',\n",
    "            'language_construct': 'Subjective Language',\n",
    "            'lit_reference'     : 'Gleich B, Creighton O, Kof L. Ambiguity detection: Towards a tool explaining ambiguity sources. InRequirements Engineering: Foundation for Software Quality: 16th International Working Conference, REFSQ 2010, Essen, Germany, June 30‚ÄìJuly 2, 2010. Proceedings 16 2010 (pp. 218-232). Springer Berlin Heidelberg.',\n",
    "        },\n",
    "        'dangerous_plural'  : {\n",
    "            'title'             : 'Dangerous Reference Plural',\n",
    "            'description'       : 'Dangerous plural with ambiguous reference.',\n",
    "            'regexp'            : '(?:\\\\ball\\\\b|\\\\beach\\\\b|\\\\bevery\\\\b) .* (?:\\\\bhis\\\\b|\\\\bher\\\\b|\\\\bits\\\\b|\\\\btheir\\\\b|\\\\bthey\\\\b)',\n",
    "            'language_construct': 'Subjective Language',\n",
    "            'lit_reference'     : 'Gleich B, Creighton O, Kof L. Ambiguity detection: Towards a tool explaining ambiguity sources. InRequirements Engineering: Foundation for Software Quality: 16th International Working Conference, REFSQ 2010, Essen, Germany, June 30‚ÄìJuly 2, 2010. Proceedings 16 2010 (pp. 218-232). Springer Berlin Heidelberg.',\n",
    "        },\n",
    "    },\n",
    "    'check_pos_regexes': {\n",
    "        'passive_ambiguity' : {\n",
    "            'title'             : 'Passive Voice Ambiguity',\n",
    "            'description'       : 'Authors should state requirements in active form, as passive conceals who is responsible for the action.',\n",
    "            'regexp'            : '\\\\b\\\\w+?¬∞V[^¬∞]*¬∞be (\\\\W[^¬∞]+?¬∞(?!VB.)[^¬∞]*¬∞[^ ]+?)* \\\\W\\\\w+?¬∞VBN¬∞\\\\w+',\n",
    "            'language_construct': 'Subjective Language',\n",
    "            'lit_reference'     : 'Gleich B, Creighton O, Kof L. Ambiguity detection: Towards a tool explaining ambiguity sources. InRequirements Engineering: Foundation for Software Quality: 16th International Working Conference, REFSQ 2010, Essen, Germany, June 30‚ÄìJuly 2, 2010. Proceedings 16 2010 (pp. 218-232). Springer Berlin Heidelberg.',\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T12:18:47.826977Z",
     "start_time": "2023-09-11T12:16:06.976224Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#############################################################| 100/100 [00:03<00:00, 29.34it/s]\n"
     ]
    }
   ],
   "source": [
    "#  Check for Subjective Language ambiguities using the lexicon defined above\n",
    "issues_ambs_found_subjective_language = check_for_ambiguities(issues, subject_language_lexicons, sample_data_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------- Issue ID: 13313275 --------------------------------------------------\n",
      "\n",
      "As a user of OpsnShift I want to be able to tell if my underlying network is providing a solid foundation for my SDN.\n",
      "\n",
      "I also want to be sure that my SDN is performing correctly.\n",
      "\n",
      "We need to make sure there is an easy way to see the node-to-node performance numbers across the cluster.¬† We also need to see if we can alert on bad underlay performance (without too many false positives) and we need to see if we can push any meaningful metrics up to insights (can we do a median and a standard distribution across nodes?)\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Dangerous Plural',\n",
      "  'index_end': 423,\n",
      "  'index_start': 420,\n",
      "  'text': 'any'},\n",
      " {'amb_type': 'Dangerous Plural',\n",
      "  'index_end': 368,\n",
      "  'index_start': 364,\n",
      "  'text': 'many'},\n",
      " {'amb_type': 'Unclear Inclusion',\n",
      "  'index_end': 449,\n",
      "  'index_start': 443,\n",
      "  'text': 'up to '}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12965273 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a user, I would like to be able to configure my when statements in the Pipeline Builder as I am crafting my Pipeline.\n",
      "\n",
      "h3. Acceptance Criteria\n",
      " # Support a when section in the sidebar for all tasks (place it at the bottom of the sidebar)\n",
      " #* See designs for the structure\n",
      " #* Support Code Assistance from ODC-4277\n",
      " #** Input can take \"Parameters or Results\" auto complete values\n",
      " #** Values can take \"Parameters, Results or Workspace's bound state\"\n",
      " # Support the When look from ODC-5152 in the Visualization when we have a when state\n",
      "\n",
      "h3. Additional Details:\n",
      "\n",
      "Docs: https://github.com/tektoncd/pipeline/blob/master/docs/pipelines.md#guard-task-execution-using-whenexpressions\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Dangerous Plural',\n",
      "  'index_end': 211,\n",
      "  'index_start': 208,\n",
      "  'text': 'all'},\n",
      " {'amb_type': 'Inside Behaviour',\n",
      "  'index_end': 230,\n",
      "  'index_start': 228,\n",
      "  'text': 'at'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12955816 --------------------------------------------------\n",
      "\n",
      "h2. User Story\n",
      "\n",
      "As a user of OpenShift\n",
      " I want to import OCI images\n",
      " So that I can use images that are built by new tools\n",
      "h2. Acceptance Criteria\n",
      " * {{oc import-image}} works with OCI images\n",
      "\n",
      "h2. Launch Checklist\n",
      "\n",
      "(?) Dependencies identified\n",
      " (?) Blockers noted and expected delivery timelines set\n",
      " (?) Design is implementable\n",
      " (?) Acceptance criteria agreed upon\n",
      " (?) Story estimated\n",
      "h2. Notes\n",
      "\n",
      "Add pertinent notes here:\n",
      " * [https://bugzilla.redhat.com/show_bug.cgi?id=1817418]\n",
      "\n",
      "----\n",
      "h2. Guiding Questions\n",
      "h3. User Story\n",
      " * Is this intended for an administrator, application developer, or other type of OpenShift user?\n",
      " * What experience level is this intended for? New, experienced, etc.?\n",
      " * Why is this story important? What problems does this solve? What benefit(s) will the customer experience?\n",
      " * Is this part of a larger epic or initiative? If so, ensure that the story is linked to the appropriate epic and/or initiative.\n",
      "\n",
      "h3. Acceptance Criteria\n",
      " * How should a customer use and/or configure the feature?\n",
      " * Are there any prerequisites for using/enabling the feature?\n",
      "\n",
      "h3. Notes\n",
      " * Is this a new feature, or an enhancement of an existing feature? If the latter, list the feature and docs reference.\n",
      " * Are there any new terms, abbreviations, or commands introduced with this story? Ex: a new command line argument, a new custom resource.\n",
      " * Are there any recommended best practices when using this feature?\n",
      " * On feature completion, are there any known issues that customers should be aware of?\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Dangerous Plural',\n",
      "  'index_end': 1030,\n",
      "  'index_start': 1027,\n",
      "  'text': 'any'},\n",
      " {'amb_type': 'Dangerous Plural',\n",
      "  'index_end': 1224,\n",
      "  'index_start': 1221,\n",
      "  'text': 'any'},\n",
      " {'amb_type': 'Dangerous Plural',\n",
      "  'index_end': 1363,\n",
      "  'index_start': 1360,\n",
      "  'text': 'any'},\n",
      " {'amb_type': 'Dangerous Plural',\n",
      "  'index_end': 1455,\n",
      "  'index_start': 1452,\n",
      "  'text': 'any'},\n",
      " {'amb_type': 'Passive Voice Ambiguity',\n",
      "  'index_end': 114,\n",
      "  'index_start': 105,\n",
      "  'text': 'are¬∞VBP¬∞be built¬∞VBN¬∞build'},\n",
      " {'amb_type': 'Passive Voice Ambiguity',\n",
      "  'index_end': 572,\n",
      "  'index_start': 557,\n",
      "  'text': 'Is¬∞VBZ¬∞be this¬∞DT¬∞this intended¬∞VBN¬∞intend'},\n",
      " {'amb_type': 'Passive Voice Ambiguity',\n",
      "  'index_end': 690,\n",
      "  'index_start': 677,\n",
      "  'text': 'is¬∞VBZ¬∞be this¬∞DT¬∞this intended¬∞VBN¬∞intend'},\n",
      " {'amb_type': 'Passive Voice Ambiguity',\n",
      "  'index_end': 924,\n",
      "  'index_start': 915,\n",
      "  'text': 'is¬∞VBZ¬∞be linked¬∞VBN¬∞link'},\n",
      " {'amb_type': 'Passive Voice Ambiguity',\n",
      "  'index_end': 1321,\n",
      "  'index_start': 1259,\n",
      "  'text': 'Are¬∞VBP¬∞be there¬∞EX¬∞there any¬∞DT¬∞any new¬∞JJ¬∞new terms¬∞NNS¬∞term '\n",
      "          ',¬∞,¬∞, abbreviations¬∞NNS¬∞abbreviation ,¬∞,¬∞, or¬∞CC¬∞or '\n",
      "          'commands¬∞NNS¬∞command introduced¬∞VBN¬∞introduce'},\n",
      " {'amb_type': 'Passive Voice Ambiguity',\n",
      "  'index_end': 1424,\n",
      "  'index_start': 1402,\n",
      "  'text': 'Are¬∞VBP¬∞be there¬∞EX¬∞there any¬∞DT¬∞any recommended¬∞VBN¬∞recommend'},\n",
      " {'amb_type': 'Passive Voice Ambiguity',\n",
      "  'index_end': 1514,\n",
      "  'index_start': 1496,\n",
      "  'text': 'are¬∞VBP¬∞be there¬∞EX¬∞there any¬∞DT¬∞any known¬∞VBN¬∞know'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14297867 --------------------------------------------------\n",
      "\n",
      "h2. User Story:\n",
      "\n",
      "As an installer team member, I want to be able to:\n",
      " * provide knowledge, guidance, and PR reviews to the IBM Cloud work.\n",
      "\n",
      "so that I can achieve\n",
      " * enabling the addition of the IBM Cloud provider to the installer.\n",
      "\n",
      "h2. Acceptance Criteria:\n",
      "\n",
      "Description of criteria:\n",
      " * I am responding to questions regarding the installer (including CI) in a timely manner.\n",
      " * I am reviewing and providing feedback to installer and release PRs in a timely manner.\n",
      " * I am approving installer PRs in a timely manner.\n",
      "\n",
      "h2. (optional) Out of Scope:\n",
      "\n",
      "We are not writing any code as part of this story.\n",
      "h2. Engineering Details:\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Dangerous Plural',\n",
      "  'index_end': 568,\n",
      "  'index_start': 565,\n",
      "  'text': 'any'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13353905 --------------------------------------------------\n",
      "\n",
      "Idea is to monitor defunct processes on a node and alert if we find multiple ones. This means we would require a new metric within CRI-O to check for zombie processes and trace them back to the managed containers. Beside that, a new alert would be required within OpenShift.\n",
      "h3. Story\n",
      "\n",
      "As a cluster operator, I would like to get monitoring events for out of memory processes\n",
      "h3. {color:#151515}Definition of Done{color}\n",
      "\n",
      "Metric is integrated in CRI-O.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Passive Voice Ambiguity',\n",
      "  'index_end': 256,\n",
      "  'index_start': 245,\n",
      "  'text': 'be¬∞VB¬∞be required¬∞VBN¬∞require'},\n",
      " {'amb_type': 'Passive Voice Ambiguity',\n",
      "  'index_end': 447,\n",
      "  'index_start': 434,\n",
      "  'text': 'is¬∞VBZ¬∞be integrated¬∞VBN¬∞integrate'}]\n"
     ]
    }
   ],
   "source": [
    "# Display the ambiguites found, one issue at a time\n",
    "display_issue_ambiguities(issues_ambs_found_subjective_language, sample_data_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Ambiguit Detection: Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some example Coordination detection lexicons\n",
    "coordination_lexicons = {\n",
    "    'check_lexical': {\n",
    "        # None \n",
    "    },\n",
    "    'check_regexes': {\n",
    "        # None \n",
    "    },\n",
    "    'check_pos_regexes': {\n",
    "        'coordination_adj' : {\n",
    "            'title'         : 'Coordination of two nouns modified by an adjective',\n",
    "            'description'   : 'Following an adjective by two nouns joint by \"and\" or \"or\" makes it unclear if the adjective describes the first noun only or both nouns.',\n",
    "            'regexp'        : '(?<=\\\\s)\\\\S*¬∞JJ[^¬∞]*¬∞[\\\\S]+\\\\s[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+\\\\s(and|or)¬∞CC¬∞(and|or)\\\\s[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+',\n",
    "            'lit_reference'  : 'Yang H, Willis A, De Roeck A, Nuseibeh B. Automatic detection of nocuous coordination ambiguities in natural language requirements. InProceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering 2010 Sep 20 (pp. 53-62).',\n",
    "            'language_construct': 'Coordination'\n",
    "        },\n",
    "        'coordination_vb' : {\n",
    "            'title'         : 'Coordination of two nouns preceded by a verb',\n",
    "            'description'   : 'Following a verb by two nouns joint by \"and\" or \"or\" makes it unclear if the verb describes the first noun only or both nouns.',\n",
    "            'regexp'        : '(?<=\\\\s)\\\\S*¬∞VBN*¬∞[\\\\S]+\\\\s[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+\\\\s(and|or)¬∞CC¬∞(and|or)\\\\s[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+',\n",
    "            'lit_reference'  : 'Yang H, Willis A, De Roeck A, Nuseibeh B. Automatic detection of nocuous coordination ambiguities in natural language requirements. InProceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering 2010 Sep 20 (pp. 53-62).',\n",
    "            'language_construct': 'Coordination'\n",
    "        },\n",
    "        'coordination_nn' : {\n",
    "            'title'         : 'Coordination of two nouns preceded by another noun',\n",
    "            'description'   : 'Following a noun by two nouns joint by \"and\" or \"or\" makes it unclear if the noun describes the first noun only or both nouns.',\n",
    "            'regexp'        : 'w+?¬∞NN[^¬∞]*¬∞[\\\\S]+\\\\s([\\\\S]+¬∞IN¬∞[\\\\S]+\\\\s)*[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+\\\\s(and|or)¬∞CC¬∞(and|or)\\\\s[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+',\n",
    "            'lit_reference'  : 'Yang H, Willis A, De Roeck A, Nuseibeh B. Automatic detection of nocuous coordination ambiguities in natural language requirements. InProceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering 2010 Sep 20 (pp. 53-62).',\n",
    "            'language_construct': 'Coordination'\n",
    "        },\n",
    "        'coordination_post_nn' : {\n",
    "            'title'         : 'Coordination of two nouns followed by another noun',\n",
    "            'description'   : 'Following two nouns joint by \"and\" or \"or\" by a third noun makes the word association unclear.',\n",
    "            'regexp'        : '([\\\\S]+¬∞DT¬∞[\\\\S]+\\\\s)*([\\\\S]+¬∞JJ[^¬∞]*¬∞[\\\\S]+\\\\s)*[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+\\\\s(and|or)¬∞CC¬∞(and|or)\\\\s([\\\\S]+¬∞DT¬∞[\\\\S]+\\\\s)*([\\\\S]+¬∞JJ[^¬∞]*¬∞[\\\\S]+\\\\s)*[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+\\\\s([\\\\S]+¬∞IN¬∞[\\\\S]+\\\\s)*([\\\\S]+¬∞DT¬∞[\\\\S]+\\\\s)*([\\\\S]+¬∞JJ[^¬∞]*¬∞[\\\\S]+\\\\s)*[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+',\n",
    "            'lit_reference'  : 'Yang H, Willis A, De Roeck A, Nuseibeh B. Automatic detection of nocuous coordination ambiguities in natural language requirements. InProceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering 2010 Sep 20 (pp. 53-62).',\n",
    "            'language_construct': 'Coordination'\n",
    "        },\n",
    "        'coordination_adv' : {\n",
    "            'title'         : 'Coordination of two verbs modified by an adverb',\n",
    "            'description'   : 'Following an adverb by two verbs joint by \"and\" or \"or\" makes it unclear if the adverb describes the first verb only or both verbs.',\n",
    "            'regexp'        : '(?<=\\\\s)\\\\S*¬∞RB[^¬∞]*¬∞[\\\\S]+\\\\s[\\\\S]+¬∞VB[^¬∞]*¬∞[\\\\S]+\\\\s(and|or)¬∞CC¬∞(and|or)\\\\s[\\\\S]+¬∞VB[^¬∞]*¬∞[\\\\S]+',\n",
    "            'lit_reference'  : 'Yang H, Willis A, De Roeck A, Nuseibeh B. Automatic detection of nocuous coordination ambiguities in natural language requirements. InProceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering 2010 Sep 20 (pp. 53-62).',\n",
    "            'language_construct': 'Coordination'\n",
    "        },\n",
    "        'coordination_nn_vb' : {\n",
    "            'title'         : 'Coordination of two verbs preceded by a noun',\n",
    "            'description'   : 'Following a noun with two verbs joint by \"and\" or \"or\" makes it unclear if the noun describes the first verb only or both verbs.',\n",
    "            'regexp'        : '[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+\\\\s([\\\\S]+¬∞IN¬∞[\\\\S]+\\\\s)*([\\\\S]+¬∞JJ[^¬∞]*¬∞[\\\\S]+\\\\s)*[\\\\S]+¬∞VB[^¬∞]*¬∞[\\\\S]+\\\\s(and|or)¬∞CC¬∞(and|or)\\\\s([\\\\S]+¬∞JJ[^¬∞]*¬∞[\\\\S]+\\\\s)*[\\\\S]+¬∞VB[^¬∞]*¬∞[\\\\S]+',\n",
    "            'lit_reference'  : 'Yang H, Willis A, De Roeck A, Nuseibeh B. Automatic detection of nocuous coordination ambiguities in natural language requirements. InProceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering 2010 Sep 20 (pp. 53-62).',\n",
    "            'language_construct': 'Coordination'\n",
    "        },\n",
    "        'coordination_vb_nn' : {\n",
    "            'title'         : 'Coordination of two verbs followed by a noun',\n",
    "            'description'   : 'Following two verbs joint by \"and\" or \"or\" by a noun makes it unclear if the noun describes the second verb only or both verbs.',\n",
    "            'regexp'        : '(?<=\\\\s)\\\\S*¬∞VB[^¬∞]*¬∞[\\\\S]+\\\\s(and|or)¬∞CC¬∞(and|or)\\\\s[\\\\S]+¬∞VB[^¬∞]*¬∞[\\\\S]+\\\\s(\\\\S]+¬∞IN¬∞[\\\\S]+\\\\s)*([\\\\S]+¬∞DT¬∞[\\\\S]+\\\\s)*([\\\\S]+¬∞(JJ[^¬∞]*|VBN)¬∞[\\\\S]+\\\\s)*[\\\\S]+¬∞NN[^¬∞]*¬∞[\\\\S]+',\n",
    "            'lit_reference'  : 'Yang H, Willis A, De Roeck A, Nuseibeh B. Automatic detection of nocuous coordination ambiguities in natural language requirements. InProceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering 2010 Sep 20 (pp. 53-62).',\n",
    "            'language_construct': 'Coordination'\n",
    "        },\n",
    "        'coordination_vb_adv' : {\n",
    "            'title'         : 'Coordination of two verbs followed by an adverb',\n",
    "            'description'   : 'Following two verbs joint by \"and\" or \"or\" by an adverb makes it unclear if the adverb describes the second verb only or both verbs.',\n",
    "            'regexp'        : '(?<=\\\\s)\\\\S*¬∞VB[^¬∞]*¬∞[\\\\S]+\\\\s(and|or)¬∞CC¬∞(and|or)\\\\s[\\\\S]+¬∞VB[^¬∞]*¬∞[\\\\S]+\\\\s[\\\\S]+¬∞RB[^¬∞]*¬∞[\\\\S]+',\n",
    "            'lit_reference'  : 'Yang H, Willis A, De Roeck A, Nuseibeh B. Automatic detection of nocuous coordination ambiguities in natural language requirements. InProceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering 2010 Sep 20 (pp. 53-62).',\n",
    "            'language_construct': 'Coordination'\n",
    "        } \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#############################################################| 100/100 [00:23<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "#  Check for Coordination ambiguities using the lexicon defined above\n",
    "issues_ambs_found_coordination = check_for_ambiguities(issues, coordination_lexicons, sample_data_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------- Issue ID: 13335622 --------------------------------------------------\n",
      "\n",
      "As a release engineer\n",
      "I want to be able to attach RHCOS builds to the OCP errata\n",
      "So that I can easily include RHCOS artifacts as part of the OCP release process.\n",
      "\n",
      "*Acceptance Criteria*\n",
      "\n",
      "¬† - pipeline step is added that does the tagging of the RHCOS build uploaded to Brew\n",
      "¬† - jobspec.yaml has config knobs to control tagging and tag used\n",
      "¬† - tagging can be performed successfully with existing keytab\n",
      "¬† - tagging can be performed successfully for all arches\n",
      "\n",
      "*Notes*\n",
      "Tag values can be found at https://github.com/openshift/ocp-build-data/blob/openshift-4.7/erratatool.yml.\n",
      "Substitute the release version in the URL for 4.6, 4.7, 4.8\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Coordination of two nouns preceded by a verb',\n",
      "  'index_end': 340,\n",
      "  'index_start': 319,\n",
      "  'text': 'control¬∞VB¬∞control tagging¬∞NN¬∞tagging and¬∞CC¬∞and tag¬∞NN¬∞tag'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13282377 --------------------------------------------------\n",
      "\n",
      "As a multi-arch engineer, I would like to deploy the encryption of data storage for etcd on the available Z environment, so that the the team could conduct testing and development activities.\n",
      "\n",
      "¬†\n",
      "\n",
      "Acceptance criteria:\n",
      " * Feature has been deployed\n",
      " * Feature is available on Z environments\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Coordination of two nouns preceded by a verb',\n",
      "  'index_end': 179,\n",
      "  'index_start': 148,\n",
      "  'text': 'conduct¬∞VB¬∞conduct testing¬∞NN¬∞testing and¬∞CC¬∞and '\n",
      "          'development¬∞NN¬∞development'},\n",
      " {'amb_type': 'Coordination of two nouns followed by another noun',\n",
      "  'index_end': 190,\n",
      "  'index_start': 156,\n",
      "  'text': 'testing¬∞NN¬∞testing and¬∞CC¬∞and development¬∞NN¬∞development '\n",
      "          'activities¬∞NNS¬∞activity'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13153658 --------------------------------------------------\n",
      "\n",
      "h1. User Story\n",
      "As a developer I would like a Golang based operator to provide the flexibility we need to gather, collect, and package prometheus query results \n",
      "----\n",
      "\n",
      "h2. UX Requirements\n",
      "* n/a\n",
      "\n",
      "h2. UI Requirements\n",
      "* n/a\n",
      "\n",
      "h2. Documentation Requirements\n",
      "* n/a\n",
      "\n",
      "h2. Backend Requirements\n",
      "* This story is just to provide the scaffolding for development of the new operator design\n",
      "* The operator should make use of the existing Cost Management CRD for compatibility with the current operator\n",
      "\n",
      "h2. QE Requirements\n",
      "* N/A\n",
      "\n",
      "h2. Additional Information and Assumptions\n",
      "* https://sdk.operatorframework.io/docs/building-operators/golang/quickstart/\n",
      "* We should use the 0.19.3 version of the operator-sdk to align with 4.6 or use 1.0?\n",
      "\n",
      "\n",
      "h2. Acceptance Criteria\n",
      "* We have the basic templated structure for our operator set and ready to develop against\n",
      "* We can perform a similar flow using quay.io branches for deploying the operator\n",
      "* When the operator runs it logs the associated Cost Management custom resource\n",
      "\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Coordination of two nouns modified by an adjective',\n",
      "  'index_end': 573,\n",
      "  'index_start': 537,\n",
      "  'text': 'Additional¬∞JJ¬∞additional Information¬∞NNP¬∞Information and¬∞CC¬∞and '\n",
      "          'Assumptions¬∞NNP¬∞Assumptions'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14297867 --------------------------------------------------\n",
      "\n",
      "h2. User Story:\n",
      "\n",
      "As an installer team member, I want to be able to:\n",
      " * provide knowledge, guidance, and PR reviews to the IBM Cloud work.\n",
      "\n",
      "so that I can achieve\n",
      " * enabling the addition of the IBM Cloud provider to the installer.\n",
      "\n",
      "h2. Acceptance Criteria:\n",
      "\n",
      "Description of criteria:\n",
      " * I am responding to questions regarding the installer (including CI) in a timely manner.\n",
      " * I am reviewing and providing feedback to installer and release PRs in a timely manner.\n",
      " * I am approving installer PRs in a timely manner.\n",
      "\n",
      "h2. (optional) Out of Scope:\n",
      "\n",
      "We are not writing any code as part of this story.\n",
      "h2. Engineering Details:\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Coordination of two nouns followed by another noun',\n",
      "  'index_end': 458,\n",
      "  'index_start': 433,\n",
      "  'text': 'installer¬∞NN¬∞installer and¬∞CC¬∞and release¬∞NN¬∞release PRs¬∞NNS¬∞pr'},\n",
      " {'amb_type': 'Coordination of two verbs followed by a noun',\n",
      "  'index_end': 429,\n",
      "  'index_start': 397,\n",
      "  'text': 'reviewing¬∞VBG¬∞review and¬∞CC¬∞and providing¬∞VBG¬∞provide '\n",
      "          'feedback¬∞NN¬∞feedback'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12590235 --------------------------------------------------\n",
      "\n",
      "User Story:\n",
      "\n",
      "As a Developer I want to be able to incrementally deploy changes to my running application without building from scratch so I can see changes quickly.\n",
      "\n",
      "Acceptance Criteria:\n",
      "\n",
      "* Once the initial application is deployed changes made in the IDE should be deployed over to the containers filesystem/deployment.\n",
      "* Changes that the runtime support to pickup dynamically (i.e. html/js for JavaEE) should take effect immediatly and be seen in a browser refresh.\n",
      "* Changes that the the runtime requires a restart for (i.e. class files) should take effect when restarting the application (which should not require a rebuild from source)\n",
      "* Same workflow should work for docker and openshift applications (but each might implement it differently using oc sync or docker rsync or volume mounts etc.)\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Coordination of two nouns followed by another noun',\n",
      "  'index_end': 712,\n",
      "  'index_start': 680,\n",
      "  'text': 'docker¬∞NN¬∞docker and¬∞CC¬∞and openshift¬∞NN¬∞openshift '\n",
      "          'applications¬∞NNS¬∞application'},\n",
      " {'amb_type': 'Coordination of two nouns followed by another noun',\n",
      "  'index_end': 784,\n",
      "  'index_start': 764,\n",
      "  'text': 'sync¬∞NN¬∞sync or¬∞CC¬∞or docker¬∞NN¬∞docker rsync¬∞NN¬∞rsync'}]\n"
     ]
    }
   ],
   "source": [
    "# Display the ambiguites found, one issue at a time\n",
    "display_issue_ambiguities(issues_ambs_found_coordination, sample_data_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Ambiguit Detection: Compound Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some example Compound Nouns language detection lexicons\n",
    "compound_nouns_lexicons = {\n",
    "    'check_compound_nouns': {\n",
    "        'ambiguous_compounds'  : {\n",
    "            'title'             : 'Ambiguous Compound Nouns',\n",
    "            'description'       : 'A sequence of more than two consecutive nouns may have more than one interpretation depending on the possible associations between the words.',\n",
    "            'lit_reference'     : 'NOVEL',\n",
    "            'language_construct': 'Compound Noun'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T09:51:50.134629Z",
     "start_time": "2023-09-11T09:51:50.130010Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#############################################################| 100/100 [00:03<00:00, 31.62it/s]\n"
     ]
    }
   ],
   "source": [
    "#  Check for Compound Nouns ambiguities using the lexicon defined above\n",
    "issues_ambs_found_compound_nouns = check_for_ambiguities(issues, compound_nouns_lexicons, sample_data_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------- Issue ID: 14244911 --------------------------------------------------\n",
      "\n",
      "**USER STORY:**\n",
      "\n",
      "As a customer, I want receive alerts when credentials are inadequate¬†so that I can scale storage and capacity as needed.\n",
      "\n",
      "**DESCRIPTION:**\n",
      "\n",
      "Hemant reached out to me to see if we could integrate [https://github.com/rvanderp3/vsphere-priv-check]¬†in to the vSphere problem detector.¬†¬†\n",
      "\n",
      "**Required:**\n",
      "\n",
      "- Pull request¬†\n",
      "\n",
      "**Nice to have:**\n",
      "\n",
      "**ACCEPTANCE CRITERIA:**\n",
      "\n",
      "- Must check that credentials required for machinesets to scale up/down and storage to provision\n",
      "\n",
      "**ENGINEERING DETAILS:**\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 295,\n",
      "  'index_start': 271,\n",
      "  'text': 'vSphere problem detector'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13380031 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a user,\n",
      "h3. Acceptance Criteria\n",
      " # <criteria>\n",
      "\n",
      "h3. Additional Details:\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 69,\n",
      "  'index_start': 32,\n",
      "  'text': 'Acceptance Criteria\\n # <criteria>\\n\\nh3'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14260410 --------------------------------------------------\n",
      "\n",
      "As a user of OpenShift Pipelines, I would like to install unreleased builds of the product on my cluster. It was partially implemented but it's not in ideal state\n",
      "\n",
      "h3. Acceptance criteria\n",
      "* For every Pipelines build (pre-stage and stage), all product images are mirrored to https://quay.io/organization/openshift-pipeline/\n",
      "* For every Pipelines build (pre-stage and stage), custom catalog source and image content source policy are uploaded to https://artifacts.tekton.codereadyqe.com/ to a first-level directory\n",
      "* URL to given build provides as much useful info as possible, mainly product version, OpenShift version and build number\n",
      "* The actual deployments resides in pipelines-ci namespace on CICD cluster\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 395,\n",
      "  'index_start': 374,\n",
      "  'text': 'custom catalog source'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 427,\n",
      "  'index_start': 400,\n",
      "  'text': 'image content source policy'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 693,\n",
      "  'index_start': 671,\n",
      "  'text': 'pipelines-ci namespace'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14295017 --------------------------------------------------\n",
      "\n",
      "h2. *User Story*\n",
      "\n",
      "As a user I might want to add additional trusted CA bunlde to my cluster and expect that CCMs would respect it.\n",
      "h2. *Background*\n",
      "\n",
      "On on-prem platforms, such as ASH or OpenStack for its endpoints internally or self-signed SSL certificates might be used. Which would lead to connection issues between CCMs and underlying platform due to no trust to this custom CAs.\n",
      "\n",
      "Also, cluster-wide proxy with such certs might be configured which also would lead to the same problem.\n",
      "\n",
      "Openshift doc prescribe to use cluster scope Proxy object for such configuration:\n",
      "[https://docs.openshift.com/container-platform/4.8/networking/configuring-a-custom-pki.html#certificate-injection-using-operators_configuring-a-custom-pki]\n",
      "Because this process heavily rely on `cluster-network-operator`, it is not possible to leverage it for CCCMO/CCMs at the moment due to its role in cluster bootsrap process.\n",
      "h2. *Steps*\n",
      " * Add separate controller which replicates necessary bits of ca config syncing to CCCMO\n",
      "\n",
      "h2. *Stakeholders*\n",
      " * On-prem platforms users\n",
      "\n",
      "h2. *Definition of Done*\n",
      " * Controller for syncing additional-ca-bundle should be implemented in CCCMO repo\n",
      " * Tests should be there\n",
      " * Controller description and this change background should be briefly documented in CCCMO repo\n",
      "\n",
      " * *Docs*\n",
      "No special requirements there, users should follow already desribed procedure:\n",
      "[https://docs.openshift.com/container-platform/4.8/networking/configuring-a-custom-pki.html]\n",
      "\n",
      " * *Testing*\n",
      "see this installer bug for more context:\n",
      "[https://bugzilla.redhat.com/show_bug.cgi?id=2010921]\n",
      "\n",
      " * Installation on platform with custom certificates on endpoints should be performed succesfully.\n",
      " * Adding and removing additional trust bundle to proxy object should be handled correctly (see [docs/dev/trusted_ca_bundle_sync.md|https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/master/docs/dev/trusted_ca_bundle_sync.md] in CCCMO repo for details)\n",
      " * Configuration with cluster-wide proxy with non-system-trusted root CAs should be handle correctly\n",
      " * Configuration with additional CA without any proxy settings should be correctly handled by ccms.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 788,\n",
      "  'index_start': 764,\n",
      "  'text': 'cluster-network-operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 897,\n",
      "  'index_start': 873,\n",
      "  'text': 'cluster bootsrap process'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14248263 --------------------------------------------------\n",
      "\n",
      "*User Story:*\n",
      "\n",
      "*As an* _Approver._\n",
      "\n",
      "*I want* to know when the system is processing my actions.\n",
      "\n",
      "*So that* I cannot interrupt it while it is working and submit data multiple times.\n",
      "\n",
      "¬†\n",
      "\n",
      "*Acceptance Criteria:*\n",
      "\n",
      "*Given* the user is submitting data to the back end,\n",
      "\n",
      "*When* they hit an \"Accept\" or \"Decline\" button,\n",
      "\n",
      "*Then* feedback is given that the system is processing\n",
      "\n",
      "*And* the buttons are disabled, preventing multiple submissions\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 13,\n",
      "  'index_start': 1,\n",
      "  'text': 'User Story:*'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 309,\n",
      "  'index_start': 282,\n",
      "  'text': 'Accept\" or \"Decline\" button'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14208234 --------------------------------------------------\n",
      "\n",
      "As a containers-team developer, I want human-understandable multi-arch build automation, so that features can be added and maintained long-term.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 30,\n",
      "  'index_start': 5,\n",
      "  'text': 'containers-team developer'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12736232 --------------------------------------------------\n",
      "\n",
      "As a Java service developer\n",
      "I want to use the w3c trace context standard\n",
      "So that all my services can exchange trace context in an interoperable way\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 27,\n",
      "  'index_start': 5,\n",
      "  'text': 'Java service developer'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 72,\n",
      "  'index_start': 46,\n",
      "  'text': 'w3c trace context standard'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13400268 --------------------------------------------------\n",
      "\n",
      "h2. User Story\n",
      "\n",
      "As a cluster admin\n",
      "I want the cluster storage operator to install the shared resources CSI driver\n",
      "So that I can test the shared resources CSI driver on my cluster\n",
      "\n",
      "h2. Acceptance Criteria\n",
      "\n",
      "* Cluster storage operator uses image references to resolve the csi-driver-shared-resource-operator and all images needed to deploy the csi driver.\n",
      "* Shared resources CSI driver is installed when the cluster enables the {{CSIDriverSharedResources}} feature gate, OR\n",
      "* Shared resource CSI driver is installed when the cluster enables the {{TechPreviewNoUpgrade}} feature set\n",
      "* CI ensures that if the {{TechPreviewNoUpgrade}} feature set is enabled on the cluster, the shared resource CSI driver is deployed and functions correctly.\n",
      "\n",
      "h2. Docs Impact\n",
      "\n",
      "Docs will need to identify how to install the shared resources CSI driver (by enabling the tech preview feature set)\n",
      "\n",
      "h2. Notes\n",
      "\n",
      "Tasks:\n",
      "- Add the Share APIs (SharedSecret, SharedConfigMap) to openshift/api\n",
      "- Generate clients in openshift/client-go for Share APIs\n",
      "- Update the CSI driver name used in the enum for the ClusterCSIDriver custom resource.\n",
      "- Generate custom resource definitions and include it in the deployment YAMLs for the shared resource operator\n",
      "- Add YAML deployment manifests for the shared resource operator to the cluster storage operator (include necessary RBAC)\n",
      "- Ensure cluster storage operator has permission to create custom resource definitions\n",
      "- Enhance the cluster storage operator to install the shared resource CSI driver only when the cluster enables the {{CSIDriverSharedResources}} feature gate\n",
      "\n",
      "Note that to be able to test all of this on any cloud provider, we need STOR-616 to be implemented. We can work around this by making the CSI driver installable on AWS or GCP for testing purposes.\n",
      "\n",
      "The cluster storage operator has cluster-admin permissions. However, no other CSI driver managed by the operator includes a CRD for its API.\n",
      "\n",
      "See https://issues.redhat.com/browse/BUILD-159?focusedCommentId=16360509&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16360509\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 70,\n",
      "  'index_start': 46,\n",
      "  'text': 'cluster storage operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 231,\n",
      "  'index_start': 207,\n",
      "  'text': 'Cluster storage operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 304,\n",
      "  'index_start': 269,\n",
      "  'text': 'csi-driver-shared-resource-operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 499,\n",
      "  'index_start': 427,\n",
      "  'text': 'CSIDriverSharedResources}} feature gate, OR\\n'\n",
      "          '* Shared resource CSI driver'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 698,\n",
      "  'index_start': 679,\n",
      "  'text': 'resource CSI driver'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 758,\n",
      "  'index_start': 741,\n",
      "  'text': 'Docs Impact\\n\\nDocs'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 874,\n",
      "  'index_start': 845,\n",
      "  'text': 'tech preview feature set)\\n\\nh2'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 978,\n",
      "  'index_start': 956,\n",
      "  'text': 'api\\n- Generate clients'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1045,\n",
      "  'index_start': 1030,\n",
      "  'text': 'CSI driver name'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1103,\n",
      "  'index_start': 1071,\n",
      "  'text': 'ClusterCSIDriver custom resource'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1143,\n",
      "  'index_start': 1116,\n",
      "  'text': 'custom resource definitions'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1247,\n",
      "  'index_start': 1198,\n",
      "  'text': 'resource operator\\n- Add YAML deployment manifests'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1312,\n",
      "  'index_start': 1288,\n",
      "  'text': 'cluster storage operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1371,\n",
      "  'index_start': 1340,\n",
      "  'text': 'Ensure cluster storage operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1424,\n",
      "  'index_start': 1397,\n",
      "  'text': 'custom resource definitions'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1463,\n",
      "  'index_start': 1439,\n",
      "  'text': 'cluster storage operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1505,\n",
      "  'index_start': 1486,\n",
      "  'text': 'resource CSI driver'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1809,\n",
      "  'index_start': 1785,\n",
      "  'text': 'cluster storage operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1839,\n",
      "  'index_start': 1814,\n",
      "  'text': 'cluster-admin permissions'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13424143 --------------------------------------------------\n",
      "\n",
      "As a podman developer, I my changes run against the gitlab runner tests, so that I know they always pass.\n",
      "\n",
      "¬†\n",
      "\n",
      "Ref:¬†[https://gitlab.com/gitlab-org/gitlab-runner/-/issues/27270#note_499585550https://docs.google.com/document/d/1kW7iKAthX9H_n4RAOdcI-q4fRfzQWT7LqT10rJHWQfs/edit#|https://docs.google.com/document/d/1kW7iKAthX9H_n4RAOdcI-q4fRfzQWT7LqT10rJHWQfs/edit#]\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 71,\n",
      "  'index_start': 52,\n",
      "  'text': 'gitlab runner tests'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12776171 --------------------------------------------------\n",
      "\n",
      "As a developer I would like to go to keycloak.org/kubernetes where I can\n",
      " \n",
      "* Get Started Easily by Installing KC into any Kube cluster but specifically minikube and minishift\n",
      "* Then I need the basics of how to run \"Hello World\" and secure a Spring Boot app with KC\n",
      "* Then I need how to perform a social login with Twitter\n",
      "\n",
      "all in easy to follow steps that the average enterprise developer can grok.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 256,\n",
      "  'index_start': 241,\n",
      "  'text': 'Spring Boot app'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14252036 --------------------------------------------------\n",
      "\n",
      "h2. *User Story*\n",
      "\n",
      "As a [user|developer|<other>] I want [some goal] so that¬†[some reason]\n",
      "\n",
      "Revive an outdated CAPI fork [https://github.com/openshift/cluster-api.]¬†\n",
      " * push upstream changes to master\n",
      " * add specific makefile and Dockerfile to build resources in OCP.\n",
      " * create prow jobs to build images(example: [https://github.com/openshift/release/pull/18569/files)]\n",
      " * set up automatic upstream rebases(https://github.com/openshift/release/blob/master/ci-operator/config/shiftstack/merge-bot/shiftstack-merge-bot-main.yaml#L32-L59)\n",
      "\n",
      "h2. *Background*\n",
      "\n",
      "_<Describes the context or background related to this story>_\n",
      "h2. *Steps*\n",
      " * <Add steps to complete this card if appropriate>\n",
      "\n",
      "h2. *Stakeholders*\n",
      " * <Who is interested in this/where did they request this>\n",
      "\n",
      "h2. *Definition of Done*\n",
      " * <Add items that need to be completed for this card>\n",
      "\n",
      " * *Docs*\n",
      "\n",
      " * <Add docs requirements for this card>\n",
      "\n",
      " * *Testing*\n",
      "\n",
      " * <Explain testing that will be added>\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 537,\n",
      "  'index_start': 388,\n",
      "  'text': 'upstream '\n",
      "          'rebases(https://github.com/openshift/release/blob/master/ci-operator/config/shiftstack/merge-bot/shiftstack-merge-bot-main.yaml#L32-L59)\\n'\n",
      "          '\\n'\n",
      "          'h2'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13300580 --------------------------------------------------\n",
      "\n",
      "¬†\n",
      " ¬†\n",
      "h4. Description\n",
      "\n",
      "Use Cases:\n",
      " * Accounting for users of a OCP cluster to make a correct accounting to their internal customers. Understand, what capacity they are using and how much they might have to pay for the applications they ran on the environment.\n",
      "\n",
      "¬†\n",
      " * As a Service Provider I want to plan the capacity of my environment and I'm allowed to plan to extend the environment in time due to the data the Metering allows.\n",
      "\n",
      "Misc:\n",
      " * Metering is announced to be deprecated¬† - Alternative for the first topic seems to be cost management operator\n",
      " * Alternative for the second partially Prometheus....\n",
      " * Further discussions needed\n",
      "\n",
      "¬†\n",
      "\n",
      "Open Questions:\n",
      " # Multi-tenency and data isolation of accounting data & performance data\n",
      " # Is it possible to externalize Monitoring information so that a outside tool can use that data to perform costmanagement or Monitoring of the environment.\n",
      " # What tools are needed for the cost management operator? (Metering had hive & presto)\n",
      "\n",
      "Relevant Links\n",
      " Old docs: [https://access.redhat.com/documentation/en-us/openshift_container_platform/4.5/html/getting_started_with_cost_management/assembly_adding_sources_cost#assembly_adding_ocp_sources]\n",
      " Q&A: [https://source.redhat.com/groups/public/cost/cost_management_wiki/cost_management_questions_and_answers_qa]\n",
      "\n",
      "¬†\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 31,\n",
      "  'index_start': 9,\n",
      "  'text': 'Description\\n\\nUse Cases'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 548,\n",
      "  'index_start': 524,\n",
      "  'text': 'cost management operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 689,\n",
      "  'index_start': 655,\n",
      "  'text': '# Multi-tenency and data isolation'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 942,\n",
      "  'index_start': 918,\n",
      "  'text': 'cost management operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 998,\n",
      "  'index_start': 974,\n",
      "  'text': 'Relevant Links\\n Old docs'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12618725 --------------------------------------------------\n",
      "\n",
      "{code}\n",
      "\n",
      "Feature: Social registration\n",
      "\n",
      "  As a visitor on the developers.redhat.com website,\n",
      "  I want to register using my Github account,\n",
      "  So that I can use RHD services.\n",
      "\n",
      "  Scenario: 1 - Registration using GitHub login which contains all mandatory information (first name, last name, unique email)\n",
      "    Given I am on the Registration page\n",
      "    When I register a new account using my GitHub account\n",
      "    Then I should be registered and logged in\n",
      "\n",
      "  Scenario: 2 - Registration using GitHub login which contains all mandatory information (first name, last name, but email already exists). User links GitHub account to existing account\n",
      "    Given I am on the Registration page\n",
      "    When I try to link a GitHub account to an existing account\n",
      "    Then I should see a warning that the email is already registered\n",
      "     And I click on the Send Verification email button\n",
      "    Then I should receive an email containing a verify email link\n",
      "     And I navigate to the verify email link\n",
      "    Then I should be registered and logged in\n",
      "\n",
      "  Scenario: 3 - Registration using GitHub login which contains all mandatory information (first name, last name, but email already exists). User changes email to create new account.\n",
      "    Given I am on the Registration page\n",
      "    When I try to link a GitHub account to an existing account\n",
      "    Then I should see a warning that the email is already registered\n",
      "     And I select to Choose another email\n",
      "     And I complete the required additional information with a new email address\n",
      "    Then I should be registered and logged in\n",
      "\n",
      "  Scenario: 4 - Registration using GitHub login which doesn't contain some mandatory information (first name, last name), email is unique. User is asked to fill in mandatory info during login.\n",
      "    Given I am on the Registration page\n",
      "    When I register a new account using a GitHub account that contains missing profile information\n",
      "    Then I should be asked to fill in mandatory information with a message \"In creating your Red Hat Developers user profile, it looks like we were not able to get some required information. Fill in the following fields to finalize your user profile please.\"\n",
      "\n",
      "  Scenario: 5 - Registration using GitHub login which doesn't contain some mandatory the information (first name, last name), email already exists. User links social provider to existing account\n",
      "    Given I am on the Registration page\n",
      "    When I try to register using a GitHub account that contains missing profile information$ with an existing RHD registered email\n",
      "      And I complete the required additional information\n",
      "    Then I should see a warning that the email is already registered\n",
      "      And I click on the Send Verification email button\n",
      "    Then I should receive an email containing a verify email link\n",
      "      And I navigate to the verify email link\n",
      "    Then I should be registered and logged in\n",
      "\n",
      "  Scenario: 6 - Registration using GitHub login which doesn't contain some mandatory information (first name, last name), email that already exists. User changes email to create new account.\n",
      "    Given I am on the Registration page\n",
      "    When I try to register using a GitHub account that contains missing profile information$ with an existing RHD registered email\n",
      "    Then I should be asked to fill in mandatory information with a message \"In creating your Red Hat Developers user profile, it looks like we were not able to get some required information. Fill in the following fields to finalize your user profile please.\"\n",
      "     And I complete the required additional information\n",
      "    Then I should see a warning that the email is already registered\n",
      "     And I select to Choose another email\n",
      "     And I complete the required additional information with a new email address\n",
      "    Then I should be registered and logged in\n",
      "\n",
      "{code}\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 856,\n",
      "  'index_start': 826,\n",
      "  'text': 'Send Verification email button'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 922,\n",
      "  'index_start': 905,\n",
      "  'text': 'verify email link'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 967,\n",
      "  'index_start': 950,\n",
      "  'text': 'verify email link'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1173,\n",
      "  'index_start': 1155,\n",
      "  'text': 'User changes email'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1995,\n",
      "  'index_start': 1964,\n",
      "  'text': 'Red Hat Developers user profile'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2678,\n",
      "  'index_start': 2648,\n",
      "  'text': 'Send Verification email button'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2744,\n",
      "  'index_start': 2727,\n",
      "  'text': 'verify email link'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2790,\n",
      "  'index_start': 2773,\n",
      "  'text': 'verify email link'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 3324,\n",
      "  'index_start': 3293,\n",
      "  'text': 'Red Hat Developers user profile'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12790865 --------------------------------------------------\n",
      "\n",
      "As a Keycloak Developer I want an automated process that can create a report for a Merge master run.\n",
      "\n",
      "Acceptance criteria:\n",
      "\n",
      "* Includes:\n",
      "** Results from stage branch script, i.e. the information what PRs failed to rebase (if any)\n",
      "** Results from build pipeline\n",
      "** Results from test pipelines (Keycloak and RH-SSO)\n",
      "** Any failures that occurred during the whole Merge master process\n",
      "* Prepares structured HTML report\n",
      "* Either directly sends the email to keycloak-team ML or puts the report into some env variable for Jenkins to send it\n",
      "* All files like pipeline definitions should be added to keycloak/keycloak-pipeline/merge-master\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 99,\n",
      "  'index_start': 83,\n",
      "  'text': 'Merge master run'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 171,\n",
      "  'index_start': 152,\n",
      "  'text': 'stage branch script'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 380,\n",
      "  'index_start': 360,\n",
      "  'text': 'Merge master process'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 468,\n",
      "  'index_start': 452,\n",
      "  'text': 'keycloak-team ML'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 630,\n",
      "  'index_start': 591,\n",
      "  'text': 'keycloak/keycloak-pipeline/merge-master'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12995809 --------------------------------------------------\n",
      "\n",
      "h1. User Story\n",
      "As an RBAC developer I want to know which OpenShift cluster resources are available from cost management so that I can display the appropriate resources in the RBAC UI.\n",
      "----\n",
      "\n",
      "h2. Backend Requirements\n",
      "* This is the task to create the /api/cost-management/v1/resource-types/openshift-clusters/ from the openapi spec yaml in COST-453\n",
      "\n",
      "h2. QE Requirements\n",
      "* Does QE need to complete specific work for this user story?\n",
      "\n",
      "h2. Additional Information and Assumptions\n",
      "* Anything else here\n",
      "\n",
      "h2. Acceptance Criteria\n",
      "* \n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 84,\n",
      "  'index_start': 57,\n",
      "  'text': 'OpenShift cluster resources'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 333,\n",
      "  'index_start': 316,\n",
      "  'text': 'openapi spec yaml'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13429648 --------------------------------------------------\n",
      "\n",
      "MON-1749 implements a feature where we users can configure CMO to disable the local Alertmanager. As a follow up, we docs for 4.9 need to be updated to reflect this option.\n",
      "\n",
      "Example configuration for disabling Alertmanager:\n",
      "\n",
      "¬†\n",
      "{code:java}\n",
      "apiVersion: v1\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  name: cluster-monitoring-config\n",
      "  namespace: openshift-monitoring\n",
      "data:\n",
      "¬† config.yaml: |\n",
      "¬† ¬† alertmanagerMain:\n",
      "¬† ¬† ¬† enabled: false\n",
      "{code}\n",
      "¬†\n",
      "A/C\n",
      "* Document written that explains how to disable the local alertmanager.\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 325,\n",
      "  'index_start': 296,\n",
      "  'text': 'monitoring-config\\n  namespace'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 352,\n",
      "  'index_start': 327,\n",
      "  'text': 'openshift-monitoring\\ndata'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14199663 --------------------------------------------------\n",
      "\n",
      "h2. *User Story*\n",
      "\n",
      "As a OpenShift¬†developer¬†I want to be able to prevent a Machine from being deleted¬†so that I can ensure my application is safely backed up/migrated to a new host before the Machine is taken away from us.\n",
      "h2. *Background*\n",
      "\n",
      "To allow other components such as storage or etcd to safely handle a Machine going away, we need a way for them to pause the machine removal process until they have completed their shutdown procedure.\n",
      "h2. *Steps*\n",
      " * Create a new enhancement based on¬†https://github.com/kubernetes-sigs/cluster-api/blob/master/docs/proposals/20200602-machine-deletion-phase-hooks.md\n",
      "\n",
      "h2. *Stakeholders*\n",
      " * Cluster Infrastructure\n",
      " * Etcd\n",
      " * OSD\n",
      " * Storage(?)\n",
      "\n",
      "h2. *Definition of Done*\n",
      " * Enhancement proposal is merged\n",
      "\n",
      " * *Docs*\n",
      "\n",
      " * N/A\n",
      "\n",
      " * *Testing*\n",
      "\n",
      " * N/A\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 388,\n",
      "  'index_start': 365,\n",
      "  'text': 'machine removal process'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13380062 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a user, I want to store my delivery pipelines in a Git repository as the source of truth and execute the pipeline on OpenShift on Git events, so that I can version and trace changes to the delivery pipelines in Git.\n",
      "h3. Use Cases\n",
      " * Developer can see the list of Git repositories that are added to the namespace for pipeline-as-code execution\n",
      " * Developer can navigate from the Console to the Git repository on the Git provider\n",
      " * For each Git repository, developer can see the details of the last pipeline execution and the commit id that triggered it with possibility to navigating to the Git commit in the Git provider\n",
      " * Developer can see the list of pipelinerun executions related to a Git repository in a chronological order and the commit id that triggered each\n",
      "\n",
      "h3. Acceptance Criteria\n",
      " # As a user, looking at the Pipelines page in the Developer Console, I should be able to see a list of (a) Git repositories that are added to the namespace for PAC execution AND (b) all pipelines in the namespace\n",
      " # As a user, I should be able to navigate to a details page of the git repo.\n",
      " ## This details page should provide access to (a) details of the git repo and (b) a list of pipeline runs.\n",
      " ## This PLR tab should show additional information than the typical PLR List view, including SHA (commit id), commit message, branch & trigger type\n",
      " # As a user, when looking at a Pipeline Run Details page, if associate with a git repo (PAC),\n",
      " ## Indicate that it's from a specific git repo rather than a PL resource\n",
      " ## Include the SHA (commit id), commit message, branch & trigger type\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 768,\n",
      "  'index_start': 759,\n",
      "  'text': 'commit id'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1127,\n",
      "  'index_start': 1108,\n",
      "  'text': '# This details page'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1296,\n",
      "  'index_start': 1283,\n",
      "  'text': 'PLR List view'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1420,\n",
      "  'index_start': 1395,\n",
      "  'text': 'Pipeline Run Details page'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12884086 --------------------------------------------------\n",
      "\n",
      "As an App SRE I require that Quay Build Service have a managed upgrade procedure, either an operator or a CD pipeline so that an SRE can apply upgrades to the service in a uniform predictable fashion\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 47,\n",
      "  'index_start': 29,\n",
      "  'text': 'Quay Build Service'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13345843 --------------------------------------------------\n",
      "\n",
      "As a cluster administrator, I want the GCP PD CSI Migration enabled when I enable the TechPreviewNoUpgrade featureSet.\n",
      "\n",
      "Update the TechPreviewNoUpgrade featureSet at [https://github.com/openshift/api/blob/a99ffa1cac6709edf8f502b16890b16f9a557e00/config/v1/types_feature.go#L31] to include the GCP PD CSI Migration feature gate flags.\n",
      "\n",
      "Update the carry patch to enable GCE CSI migration in A/D controller.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 59,\n",
      "  'index_start': 39,\n",
      "  'text': 'GCP PD CSI Migration'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 332,\n",
      "  'index_start': 293,\n",
      "  'text': 'GCP PD CSI Migration feature gate flags'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 385,\n",
      "  'index_start': 368,\n",
      "  'text': 'GCE CSI migration'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13260958 --------------------------------------------------\n",
      "\n",
      "*User Story*\n",
      "\n",
      "As a developer using OpenShift\n",
      " I want more control over the directory and file layout for data mounted by the Projected Resource CSI driver\n",
      "\n",
      "*Acceptance Criteria*\n",
      " # Minimally define/document new volumeAttribute key / values that the driver will inspect to control\n",
      " # Or if other API level ideas surface during planning, entertain those\n",
      " Consensus on default behavior\n",
      "\n",
      "*Notes*\n",
      " # Today, the driver alway creates subdirs for secret/configmap, namespace:name, before creating the file associated with each key/value pair from the ConfigMap or Secret\n",
      " # Do we have to entertain all the symlink hoopla that k8s does ... i.e. [https://github.com/kubernetes/kubernetes/blob/master/pkg/volume/util/atomic_writer.go]; in theory, since our controller is dynamically updating the content, seems like the answer is \"no\"\n",
      " # the subdirs make it easier to deal with keys within a secret getting removed (you can just delete/recreate the entire subdir), vs. having to track files that no longer correspond to keys in the latest version of the secret / configmap\n",
      " # Decision needs to be made on the default behavior ... perhaps the current subdir approach is not the correct default¬†\n",
      " # confirmed 2 volume mounts cannot use the same mount path\n",
      " # how do subpath's work with our stuff;¬† [https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath]\n",
      " # also need to determine support about MountPropagationMode and SubPathExpr\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 154,\n",
      "  'index_start': 125,\n",
      "  'text': 'Projected Resource CSI driver'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 310,\n",
      "  'index_start': 295,\n",
      "  'text': 'API level ideas'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1073,\n",
      "  'index_start': 1052,\n",
      "  'text': 'configmap\\n # Decision'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1245,\n",
      "  'index_start': 1232,\n",
      "  'text': 'mount path\\n #'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12790909 --------------------------------------------------\n",
      "\n",
      "As a Keycloak Developer I want a script that can get the approved PRs applied into the stage branch, so that it can be rebased into master.\n",
      "\n",
      "Acceptance criteria:\n",
      "\n",
      "* If verification passes PRs should be merged into master branches\n",
      "* Resulting in master being identical to stage branch afterwards\n",
      "* Pull request should be closed as merged\n",
      "* If verification fails an email should be sent to keycloak-team mailing list with a link to the failed job\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 414,\n",
      "  'index_start': 388,\n",
      "  'text': 'keycloak-team mailing list'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13195927 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a user, i would like to visualize resources created via kamelets (camel connectors) as Event Sources in topology view\n",
      "h3. Acceptance Criteria\n",
      " # Resources created for kamelets as KameletBinding should be visualised as Event Source\n",
      " # Should be able visualise sink to any knative destination\n",
      " # Should be able visualise sidebar resources for it\n",
      " # Should show icon as provided by kamelets under annotations `camel.apache.org/kamelet.icon:`¬†\n",
      "\n",
      "h3. Additional Details:\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 174,\n",
      "  'index_start': 142,\n",
      "  'text': 'Acceptance Criteria\\n # Resources'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 253,\n",
      "  'index_start': 238,\n",
      "  'text': 'Event Source\\n #'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 356,\n",
      "  'index_start': 329,\n",
      "  'text': 'visualise sidebar resources'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13406226 --------------------------------------------------\n",
      "\n",
      "As an Argo CD Admin/User I want to create an Argo CD instance which is enabled with OpenShift Login OOTB using Dex.¬†\n",
      "\n",
      "The first step or a prerequisite is to have an UBI based Image for Dex. This is a mandatory thing for productisation. Probably the most easiest way is to make use of Docker multi stage build.\n",
      "\n",
      "{color:#4c9aff}*Acceptance Criteria:*{color}\n",
      " # Build an UBI based Image for Dex.\n",
      " # Create an Argo CD instance using the OpenShift GitOps operator, Update the Subscription to install Dex. Replace the UBI image built in Step 1 in the Dex pod and make sure Dex is up and running.¬†\n",
      " # Configure Dex to enable OpenShift OAuth login and verify that users can login into Argo CD using their OpenShift Credentials.\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 24,\n",
      "  'index_start': 6,\n",
      "  'text': 'Argo CD Admin/User'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 61,\n",
      "  'index_start': 45,\n",
      "  'text': 'Argo CD instance'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 104,\n",
      "  'index_start': 84,\n",
      "  'text': 'OpenShift Login OOTB'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 308,\n",
      "  'index_start': 284,\n",
      "  'text': 'Docker multi stage build'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 358,\n",
      "  'index_start': 312,\n",
      "  'text': 'color:#4c9aff}*Acceptance Criteria:*{color}\\n #'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 422,\n",
      "  'index_start': 406,\n",
      "  'text': 'Argo CD instance'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 458,\n",
      "  'index_start': 433,\n",
      "  'text': 'OpenShift GitOps operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 607,\n",
      "  'index_start': 592,\n",
      "  'text': '# Configure Dex'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 639,\n",
      "  'index_start': 618,\n",
      "  'text': 'OpenShift OAuth login'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13003842 --------------------------------------------------\n",
      "\n",
      "As a user I want to deploy my application on OpenShift or Kubernetes using Quarkus dev mode in addition to see the logs in real time using kogito CLI.¬†\n",
      "\n",
      "In addition, I would like to redeploy my Kogito service instead deleting and deploying it when something went wrong as well deploy a kogito service using a local git repository or local project.\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 91,\n",
      "  'index_start': 75,\n",
      "  'text': 'Quarkus dev mode'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14293697 --------------------------------------------------\n",
      "\n",
      "h2. User Story:\n",
      "\n",
      "As an installer team member, I want to be able to:\n",
      " * provide knowledge, guidance, and PR reviews to the Alibaba work.\n",
      "\n",
      "so that I can achieve\n",
      " * enabling the addition of the Alibaba provider to the installer.\n",
      "\n",
      "h2. Acceptance Criteria:\n",
      "\n",
      "Description of criteria:\n",
      " * I am responding to questions regarding the installer (including CI) in a timely manner.\n",
      " * I am reviewing and providing feedback to installer and release PRs in a timely manner.\n",
      " * I am approving installer PRs in a timely manner.\n",
      "\n",
      "h2. (optional) Out of Scope:\n",
      "\n",
      "We are not writing any code as part of this story.\n",
      "h2. Engineering Details:\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 44,\n",
      "  'index_start': 23,\n",
      "  'text': 'installer team member'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12959706 --------------------------------------------------\n",
      "\n",
      "h1. User Story\n",
      "As a developer I want less code duplication so that it is easier to track. \n",
      "----\n",
      "\n",
      "h2. Backend Requirements\n",
      "* Consolidate the various query_table properties in the query handlers.\n",
      "\n",
      "h2. QE Requirements\n",
      "* Does QE need to complete specific work for this user story?\n",
      "\n",
      "h2. Additional Information and Assumptions\n",
      "* Anything else here\n",
      "\n",
      "{panel:title=Acceptance Criteria|borderStyle=solid|borderColor=#ccc|titleBGColor=#d7e0e0}\n",
      "||Completed||Criteria||\n",
      "| |Criteria 1|\n",
      "| |Criteria 2|\n",
      "{panel}\n",
      "\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 468,\n",
      "  'index_start': 350,\n",
      "  'text': 'title=Acceptance '\n",
      "          'Criteria|borderStyle=solid|borderColor=#ccc|titleBGColor=#d7e0e0}\\n'\n",
      "          '||Completed||Criteria||\\n'\n",
      "          '| |Criteria'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13211839 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a user,\n",
      "h3. Acceptance Criteria\n",
      " # <criteria>\n",
      "\n",
      "h3. Additional Details:\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 69,\n",
      "  'index_start': 32,\n",
      "  'text': 'Acceptance Criteria\\n # <criteria>\\n\\nh3'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14286011 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a user, I want to view all the pipelines provided by the admin for a selected builderImage and resource type combination and choose from any one of them.\n",
      "\n",
      "h3. Acceptance Criteria\n",
      "\n",
      " # List all the pipelines available for a given combination in the dropdown component.\n",
      " # Selecting a pipeline from the dropdown should update the below visualisation.\n",
      " # Edit flow scenarios should be handled (disable the dropdown if the pipeline is already generated).\n",
      " # Update e2e test cases accordingly.\n",
      "\n",
      " \n",
      "\n",
      "h3. Additional Details:\n",
      "\n",
      "We already have all the data prefetched and available in the component, so we can make use of it to power the dropdown values.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 140,\n",
      "  'index_start': 98,\n",
      "  'text': 'builderImage and resource type combination'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 390,\n",
      "  'index_start': 369,\n",
      "  'text': '# Edit flow scenarios'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 494,\n",
      "  'index_start': 480,\n",
      "  'text': 'e2e test cases'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12946718 --------------------------------------------------\n",
      "\n",
      "h1. User Story\n",
      "As a user I want to know how to use cost management's application settings so that I can enable OpenShift tags and see tagged data in the UI. \n",
      "----\n",
      "\n",
      "h2. Documentation Requirements\n",
      "* Work with engineering to gather information on how the application settings page works. \n",
      "* [~aberglun] can help and or link you to who can walk through our settings page as needed.\n",
      "\n",
      "\n",
      "h2. Additional Information and Assumptions\n",
      "* Application settings page: https://cloud.redhat.com/settings/applications/cost-management\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 89,\n",
      "  'index_start': 51,\n",
      "  'text': \"cost management's application settings\"},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 277,\n",
      "  'index_start': 252,\n",
      "  'text': 'application settings page'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13375089 --------------------------------------------------\n",
      "\n",
      "h1. *Story*\n",
      "\n",
      "As a partner I would like to be able to verify my chart without running into tooling issues.\n",
      "\n",
      "h2. *Glossary*\n",
      "\n",
      "tbd\n",
      "\n",
      "h2. *Out of scope*\n",
      "\n",
      "Go unit tests\n",
      "\n",
      "h2. *In Scope*\n",
      "\n",
      "Tests which verify chart-verifier functionality using the quay images in the same way we expect a user to use.\n",
      "\n",
      "h2. *Problem Description*\n",
      "\n",
      "Go unit test currently test the go apis but do not use the generate docker images as would a user.  This is currently a manual effort after build\n",
      "\n",
      "h2. *Approach*\n",
      "\n",
      "Test is probably a bash/python script which runs the chart verifier against various charts and check report content is as expected.\n",
      "\n",
      "h2. *Dependencies*\n",
      "\n",
      "None\n",
      "\n",
      "h2. *Edge Case*\n",
      "\n",
      "tbd\n",
      "h2. *Details*\n",
      "\n",
      "¬†\n",
      "h2. *Acceptance Criteria*\n",
      "\n",
      "Test coverage includes all use successful full report use cases of the chart-verifier:\n",
      " * using docker\n",
      " * using command line\n",
      "\n",
      "h2. *INVEST Checklist*\n",
      "\n",
      "(?) Dependencies identified\n",
      "\n",
      "(?) Blockers noted and expected delivery timelines set\n",
      "\n",
      "(?) Design is implementable\n",
      "\n",
      "(?) Acceptance criteria agreed upon\n",
      "\n",
      "(?) Story estimated\n",
      "\n",
      "h4. Legend\n",
      "\n",
      "(?) Unknown\n",
      "\n",
      "(/) Verified\n",
      "\n",
      "(x) Unsatisfied\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 130,\n",
      "  'index_start': 112,\n",
      "  'text': 'Glossary*\\n\\ntbd\\n\\nh2'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 518,\n",
      "  'index_start': 500,\n",
      "  'text': 'bash/python script'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 642,\n",
      "  'index_start': 619,\n",
      "  'text': 'Dependencies*\\n\\nNone\\n\\nh2'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 718,\n",
      "  'index_start': 683,\n",
      "  'text': 'Acceptance Criteria*\\n\\nTest coverage'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 833,\n",
      "  'index_start': 817,\n",
      "  'text': 'command line\\n\\nh2'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13426836 --------------------------------------------------\n",
      "\n",
      "Configuring a NFV deployment takes a bit of coordination and its difficult to do manually. \n",
      " As an Telco company I would like to have an easy way to deploy a VNF worker configured with the following:\n",
      " - ovs dpdk fast data-path on required interfaces\n",
      " - all worker host required drivers installed in CoreOS to access the fast data-path nics (PMD-drivers).\n",
      " - all required OpenStack resources defined (flavors, security groups, etc..)\n",
      "\n",
      "The system should consist of nfv workers which meet the requirements of the worker nodes in the below diagram: \n",
      " [https://coreos.slack.com/files/U012XCAEUKX/F028FUM6R5M/osasinfra_telco_openshift_perf_lab_.jpeg]\n",
      "\n",
      "Requirements:\n",
      " - OpenStack is configured to use OVS-DPDK for its internal OVS switches, as described in the official documentation [1]. This includes the allocation of memory hugepages, NUMA allocation , and CPU pinning on the OpenStack Compute node required for the deployment. All described in the documentation listed under [1]\n",
      " The OpenStack deployment should expose vms and flavors with the following characteristics:\n",
      " - {color:#de350b}TBD - list all the requirements for the vm to be used as an OpenShift node{color}. [2] (for example, driver installation, hugepages requirements, and other necessary software that needs to be installed and configured on the worker node so it can access the nics connected to the fast data-path.\n",
      " - The OpenShift cluster will NOT reach into the OpenStack deployment to make any configuration which require administrative privileges. All OpenStack configurations requiring administrative privileges will be done by the user.\n",
      "\n",
      "System characteristics:\n",
      " - Control-plane attaching workers to controllers will NOT be accelerated by OVS-DPDK. i.e. the OVN switches inside the CoreOS based worker which attach the pods to the control-plane will NOT implement OVS-DPDK.\n",
      " - Workers will have at least two interfaces attached to them that are exposed to the pods using a PMD ([Poll Mode Driver|https://doc.dpdk.org/guides/prog_guide/poll_mode_drv.html]) and are in turned accelerated by the OVS-DPDK switch in the OpenStack Compute node. In total the worker should have three interfaces. One attached to the cluster control-plane which will NOT be accelerated using and is exposed to the node as a virtio or host_user, two interfaces attached to low latency (fast data-path) networks, which will be exposed to the vm as host_user interfaces.\n",
      "\n",
      "limitations:\n",
      " - The network path of the control plane, which connects the workers to the controllers on the machineNetwork (for openshiftSDN), cannot be accelerated. Thus traffic flowing on the OpenShift control-plane will not be accelerated by ovs-dpdk.\n",
      "\n",
      "Acceptance Criteria:\n",
      " - Document describing how to deploy the system which may be used later by the customer as a guide for developing an operator to automate the process\n",
      " - Following the document, a system like the one shown in Fig 2, is deployed and \"testpmd\" pods can be deployed on to the workers, and performance test is executed. \"testpmd\" pods are pods that can run testpmd using the fast data-path [3].\n",
      " - Aspirational, the NICs used to attache the pods to the fast data-path are VF from an SR-IOV device which was configured by OpenStack administrator to expose them to the worker nodes.\n",
      "\n",
      "References:\n",
      " [1] [https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/html/network_functions_virtualization_planning_and_configuration_guide/part-dpdk-configure]\n",
      " [2] [https://docs.openstack.org/neutron/queens/admin/config-ovs-dpdk.html]\n",
      " [3] [https://docs.openvswitch.org/en/latest/howto/dpdk/]\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 285,\n",
      "  'index_start': 257,\n",
      "  'text': 'worker host required drivers'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 339,\n",
      "  'index_start': 325,\n",
      "  'text': 'data-path nics'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 895,\n",
      "  'index_start': 873,\n",
      "  'text': 'OpenStack Compute node'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2081,\n",
      "  'index_start': 2066,\n",
      "  'text': 'OVS-DPDK switch'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2111,\n",
      "  'index_start': 2089,\n",
      "  'text': 'OpenStack Compute node'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2204,\n",
      "  'index_start': 2183,\n",
      "  'text': 'cluster control-plane'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2358,\n",
      "  'index_start': 2325,\n",
      "  'text': 'latency (fast data-path) networks'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2635,\n",
      "  'index_start': 2612,\n",
      "  'text': 'OpenShift control-plane'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 3186,\n",
      "  'index_start': 3173,\n",
      "  'text': 'SR-IOV device'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12757919 --------------------------------------------------\n",
      "\n",
      "As a business analyst I want to create a single scenario outline/template so that I can concisely express and understand scenario test examples through the use of variable/data object placeholders.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 73,\n",
      "  'index_start': 48,\n",
      "  'text': 'scenario outline/template'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 143,\n",
      "  'index_start': 121,\n",
      "  'text': 'scenario test examples'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 196,\n",
      "  'index_start': 172,\n",
      "  'text': 'data object placeholders'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13267227 --------------------------------------------------\n",
      "\n",
      "As a user of RHCOS\n",
      "\n",
      "I want to be able to apply traffic limiting and QoS to host network interfaces\n",
      "\n",
      "So that I have more control over the network traffic profile of my host.\n",
      "\n",
      "*Acceptance Criteria*\n",
      " * RHCOS manifest updated to include `iproute-tc`\n",
      " * RHCOS builds successfully\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 98,\n",
      "  'index_start': 75,\n",
      "  'text': 'host network interfaces'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 160,\n",
      "  'index_start': 137,\n",
      "  'text': 'network traffic profile'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 213,\n",
      "  'index_start': 175,\n",
      "  'text': 'Acceptance Criteria*\\n * RHCOS manifest'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14341849 --------------------------------------------------\n",
      "\n",
      "¬†\n",
      "\n",
      "**USER STORY:**\n",
      "\n",
      "As a OpenShift developer, I want cluster actions to take less time so that costs and agility are improved. There is a possible efficiency gain in making AWS service invocations asynchronous and cached.¬†\n",
      "\n",
      "**DESCRIPTION:**\n",
      "\n",
      "More details in this PR\n",
      "[https://github.com/openshift/installer/pull/5344]\n",
      "¬†\n",
      "**Required:**\n",
      "\n",
      "Better performance without breaking compatibility for IAM User tags lookup\n",
      "\n",
      "**Nice to have:**\n",
      "\n",
      "Make improvements available to other AWS API Calls\n",
      "\n",
      "**ACCEPTANCE CRITERIA:**\n",
      "\n",
      "Keep passing tests with improved install / uninstall time\n",
      "\n",
      "**ENGINEERING DETAILS:**\n",
      "\n",
      "¬†\n",
      "\n",
      "Consider asynchronous queuing, caching and client tuning to get faster results while respecting service throttling.¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 196,\n",
      "  'index_start': 173,\n",
      "  'text': 'AWS service invocations'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 479,\n",
      "  'index_start': 466,\n",
      "  'text': 'AWS API Calls'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13153661 --------------------------------------------------\n",
      "\n",
      "h1. User Story\n",
      "As an engineer I want to be able to gather data for all of the relevant prom queries we need to make so that we can match the data we currently get with the existing cost mangement operator.\n",
      "----\n",
      "\n",
      "h2. UX Requirements\n",
      "* \n",
      "\n",
      "h2. UI Requirements\n",
      "* \n",
      "\n",
      "h2. Documentation Requirements\n",
      "* \n",
      "\n",
      "h2. Backend Requirements\n",
      "* We use our functionality for querying prometheus and collect each metric required\n",
      "\n",
      "h2. QE Requirements\n",
      "*\n",
      " \n",
      "h2. Additional Information and Assumptions\n",
      "* https://github.com/kube-reporting/metering-operator/blob/623fd48980ff516130cc9539a0131f4363504d9d/charts/openshift-metering/values.yaml\n",
      "* https://github.com/project-koku/korekuta-operator/blob/master/roles/setup/files/cm_kube_persistentvolume_labels_report_data_source.yaml\n",
      "* https://github.com/project-koku/korekuta-operator/blob/master/roles/setup/files/cm_kube_persistentvolumeclaim_labels_report_data_source.yaml\n",
      "* https://github.com/project-koku/korekuta-operator/blob/master/roles/setup/files/cm_kube_pod_labels_report_data_source.yaml\n",
      "* https://github.com/project-koku/korekuta-operator/blob/master/roles/setup/files/cm_kube_pod_persistentvolumeclaim_info_report_data_source.yaml\n",
      "* https://github.com/project-koku/korekuta-operator/blob/master/roles/setup/files/cm_kube_node_labels_report_data_source.yaml\n",
      "* kube_namespace_labels will also need to be collected and packaged (similar to node labels)\n",
      "\n",
      "h2. Acceptance Criteria\n",
      "* We collect all of the existing metrics that we get via the metering operator today. \n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 204,\n",
      "  'index_start': 181,\n",
      "  'text': 'cost mangement operator'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13272846 --------------------------------------------------\n",
      "\n",
      "h2. User Story\n",
      "\n",
      "As an OpenShift cluster admin\n",
      " I want official images for the Projected resource CSI driver\n",
      " So that I have fully certified and supported images for the projected resource CSI driver running on my cluster.\n",
      "h2. Acceptance Criteria\n",
      " * Set up delivery pipeline for Projected Resource CSI driver via ART\n",
      " * Set up delivery pipeline for the Projected Resource CSI Driver operator via ART\n",
      "\n",
      "h2. ART Checklist\n",
      "\n",
      "* Agree on the namings of the components (/)\n",
      "* Align upstream naming of components (if possible and not done)\n",
      "* Ensure component has end to end CI testing upstream\n",
      "* Ensure OWNERS file has appropriate Bugzilla components (/)\n",
      "* Create rhel-based Dockerfile that can be consumed by ART/OSBS\n",
      "* Create a build repository in Comet (/)\n",
      "* Create a delivery repository in Comet (/)\n",
      "* Request security audit from ProdSec (/)\n",
      "* File build ticket with ART (/)\n",
      "* Get images built in OSBS (/)\n",
      "* *After first build OSBS build* add openshift release labels to Dockerfiles (x)\n",
      "\n",
      "h2. Notes\n",
      "\n",
      "Open questions:\n",
      " # Will this ever be in the OCP payload? **YES**\n",
      " # -Does using CPaaS tie us to only install via OLM? Does it rule out our ability to install the driver via the OpenShift payload?-\n",
      "\n",
      "¬†\n",
      "\n",
      "We need to have CI configured in openshift/release before we begin adding images that will be managed by ART. (/)\n",
      "\n",
      "Process is detailed in [https://source.redhat.com/groups/public/atomicopenshift/atomicopenshift_wiki/guidelines_for_requesting_new_content_managed_by_ocp_art]\n",
      "\n",
      "and\n",
      "\n",
      "[https://source.redhat.com/groups/public/atomicopenshift/atomicopenshift_wiki/requesting_a_new_image_or_rpm_to_be_managed_by_art]\n",
      "\n",
      "Things we will need set up:\n",
      " # Listing in Comet\n",
      " # Adding code to dist-git. Note - we won't need CI set up for dist-git, ART takes care of this with nightly releases and CI against the payload.\n",
      " # Creating relevant Bugzilla components (or sub-components) - likely \"Shared Resources CSI Driver\" under Storage component\n",
      " # OWNERS files validated and updated\n",
      " # Submitting request to get ART\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 45,\n",
      "  'index_start': 22,\n",
      "  'text': 'OpenShift cluster admin'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 107,\n",
      "  'index_start': 78,\n",
      "  'text': 'Projected resource CSI driver'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 198,\n",
      "  'index_start': 179,\n",
      "  'text': 'resource CSI driver'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 307,\n",
      "  'index_start': 278,\n",
      "  'text': 'Projected Resource CSI driver'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 390,\n",
      "  'index_start': 352,\n",
      "  'text': 'Projected Resource CSI Driver operator'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 603,\n",
      "  'index_start': 585,\n",
      "  'text': 'Ensure OWNERS file'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 817,\n",
      "  'index_start': 795,\n",
      "  'text': 'Request security audit'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 960,\n",
      "  'index_start': 936,\n",
      "  'text': 'openshift release labels'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1896,\n",
      "  'index_start': 1869,\n",
      "  'text': 'Shared Resources CSI Driver'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1937,\n",
      "  'index_start': 1923,\n",
      "  'text': '# OWNERS files'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13407146 --------------------------------------------------\n",
      "\n",
      "h1. Story\n",
      "As an early adopter of cluster logging,\n",
      "I want an easy way to deploy candidate builds (CPaas?) to a cluster\n",
      "so I can vet features before any official release\n",
      "\n",
      "h1. Acceptance Criteria\n",
      "\n",
      "h1. Notes\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 196,\n",
      "  'index_start': 173,\n",
      "  'text': 'Acceptance Criteria\\n\\nh1'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13335825 --------------------------------------------------\n",
      "\n",
      "As a user, I want to disable SSO by editing an Argo CD CR.¬† All SSO resources and configs are removed from the cluster\n",
      "\n",
      "¬†\n",
      "\n",
      "*Acceptance Criteria*\n",
      " * argocd operator watches and reconciles ArgoCD CR.¬† It detects SSO flag has been set to false or removed.¬† Argocd operator removes all SSO resources and configs\n",
      " * add unit tests\n",
      " * works in both OCP and non-OCP cluster\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 57,\n",
      "  'index_start': 47,\n",
      "  'text': 'Argo CD CR'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 171,\n",
      "  'index_start': 148,\n",
      "  'text': 'argocd operator watches'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13417495 --------------------------------------------------\n",
      "\n",
      "USER STORY:\n",
      "As a CEE associate[s]/orginaization, I want to know about new features and how these new features impacts customers, as well as how they (customers) interact with it. We also want to know about the features development, how it was tested (or how we plan to test it), and how it is documented (or how we plan to document it). \n",
      "\n",
      "DESCRIPTION:\n",
      "CEE needs a new feature technical enablment overview (slide deck, pre-created: https://docs.google.com/presentation/d/1OL6DsHp6nt6_WX1Y76ZSu9r3dgpiNBe16LdRFgFoYTc/edit?usp=drivesdk), \n",
      "that outlines, what \"Move to CSI Drivers\" (feature or set of features) are/is being deliverd. \n",
      "CEE endevors to understand how we are testing, and how we are documenting the subject, so please coordinate with the proper functional teams to complete this effort (it is part of the definition of DONE). \n",
      "\n",
      "Required:\n",
      "- slide deck - https://docs.google.com/presentation/d/1OL6DsHp6nt6_WX1Y76ZSu9r3dgpiNBe16LdRFgFoYTc/edit?usp=drivesdk\n",
      "    - This is pre-created for you, you simply need to edit the supplied deck.\n",
      "\n",
      "Important Considerations:\n",
      "- Additional details on how to troubeshoot or debug this component/feature, should be a focal point. \n",
      "   - *Note*: all efforts you put in here, ultimatly go's to reducing the LOE needed to traiage and work bugs, \n",
      "       as CEE will be more prepared, and better equiped when filing issues, or addressing customer cases. \n",
      "\n",
      "More Details: \n",
      "For more information please see: https://docs.google.com/document/d/1-EjXidVHA4hB2XezuObfb7LY9xhLEIHijxIsj_ocQfw/edit#\n",
      "\n",
      "ACCEPTANCE CRITERIA:\n",
      "- All functions have the slide deck created by feature complete, so that CEE have them provided for review (to test/explore with nightly builds). \n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 964,\n",
      "  'index_start': 850,\n",
      "  'text': 'slide deck - '\n",
      "          'https://docs.google.com/presentation/d/1OL6DsHp6nt6_WX1Y76ZSu9r3dgpiNBe16LdRFgFoYTc/edit?usp=drivesdk'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12970735 --------------------------------------------------\n",
      "\n",
      "As a deployer, I want to provide custom SSL certificates to my deployment config in order for Quay to securely communicate with unmanaged components.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 56,\n",
      "  'index_start': 33,\n",
      "  'text': 'custom SSL certificates'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13417321 --------------------------------------------------\n",
      "\n",
      "USER STORY:\n",
      "As a CEE associate[s]/orginaization, I want to know about new features and how these new features impacts customers, as well as how they (customers) interact with it. We also want to know about the features development, how it was tested (or how we plan to test it), and how it is documented (or how we plan to document it). \n",
      "\n",
      "DESCRIPTION:\n",
      "CEE needs a new feature technical enablment overview (slide deck, pre-created: https://docs.google.com/presentation/d/1cugh0xbN3B8PCHVT7R95a56R0XIP879G6Mbg-lEmHVg/edit?usp=drivesdk), \n",
      "that outlines, what \"Routing Enhacements\" (feature or set of features) are/is being deliverd. \n",
      "CEE endevors to understand how we are testing, and how we are documenting the subject, so please coordinate with the proper functional teams to complete this effort (it is part of the definition of DONE). \n",
      "\n",
      "Required:\n",
      "- slide deck - https://docs.google.com/presentation/d/1cugh0xbN3B8PCHVT7R95a56R0XIP879G6Mbg-lEmHVg/edit?usp=drivesdk\n",
      "    - This is pre-created for you, you simply need to edit the supplied deck.\n",
      "\n",
      "Important Considerations:\n",
      "- Additional details on how to troubeshoot or debug this component/feature, should be a focal point. \n",
      "   - *Note*: all efforts you put in here, ultimatly go's to reducing the LOE needed to traiage and work bugs, \n",
      "       as CEE will be more prepared, and better equiped when filing issues, or addressing customer cases. \n",
      "\n",
      "More Details: \n",
      "For more information please see: https://docs.google.com/document/d/1-EjXidVHA4hB2XezuObfb7LY9xhLEIHijxIsj_ocQfw/edit#\n",
      "\n",
      "ACCEPTANCE CRITERIA:\n",
      "- All functions have the slide deck created by feature complete, so that CEE have them provided for review (to test/explore with nightly builds). \n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 964,\n",
      "  'index_start': 850,\n",
      "  'text': 'slide deck - '\n",
      "          'https://docs.google.com/presentation/d/1cugh0xbN3B8PCHVT7R95a56R0XIP879G6Mbg-lEmHVg/edit?usp=drivesdk'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14246728 --------------------------------------------------\n",
      "\n",
      "As a user, I want to have my own plugin repository from which I can work with plugins just as I could with the default plugin repository.\n",
      "\n",
      "¬†\n",
      "\n",
      "Definition of Done:\n",
      " * Must create a single, default repository where the default plugins can be published to¬†\n",
      "\n",
      " * Must define what a plugin repository looks like\n",
      "\n",
      "[~jgabani@redhat.com] We should work to set up a reference (default) repo that comes with the cli that also has a document that defines what a repo must look like in order to be considered a crane plugin repo, so that I could go read that document and implement it, building my own.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 136,\n",
      "  'index_start': 111,\n",
      "  'text': 'default plugin repository'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14297142 --------------------------------------------------\n",
      "\n",
      "h2. *User Story*\n",
      "\n",
      "As a developer I want to communicate CCCMO operator status clearly over cluster-operator resource\n",
      "\n",
      "h2. *Background*\n",
      "Currently, we have a couple of controllers (such as config map syncer), which knows nothing about cluster-operator resource. Related logic bits now tied to `ClusterOperatorReconciler`, need commonize it and use for other controllers.\n",
      "\n",
      "h2. *Steps*\n",
      " * Make cluster-operator handling bits more common for being able to use it in other controllers\n",
      " * report possible problems from  other controllers to cluster-operator status\n",
      "\n",
      "h2. *Stakeholders*\n",
      " * CCCMO developers\n",
      "\n",
      "h2. *Definition of Done*\n",
      " see steps\n",
      "\n",
      " * *Docs*\n",
      "\n",
      "Nothing, it's supposed to be internal change\n",
      "\n",
      " * *Testing*\n",
      "Existing tests should be sufficient\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 76,\n",
      "  'index_start': 55,\n",
      "  'text': 'CCCMO operator status'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 119,\n",
      "  'index_start': 90,\n",
      "  'text': 'cluster-operator resource\\n\\nh2'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 203,\n",
      "  'index_start': 186,\n",
      "  'text': 'config map syncer'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 257,\n",
      "  'index_start': 232,\n",
      "  'text': 'cluster-operator resource'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 419,\n",
      "  'index_start': 389,\n",
      "  'text': 'cluster-operator handling bits'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 560,\n",
      "  'index_start': 533,\n",
      "  'text': 'cluster-operator status\\n\\nh2'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 600,\n",
      "  'index_start': 580,\n",
      "  'text': 'CCCMO developers\\n\\nh2'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13321900 --------------------------------------------------\n",
      "\n",
      "As a cluster administrator,\n",
      "\n",
      "I want OpenShift to include a recent CoreDNS version,\n",
      "\n",
      "so that I have the latest available performance and security fixes.\n",
      "\n",
      "¬†\n",
      "\n",
      "We should strive to follow upstream CoreDNS releases by bumping [openshift/coredns|https://github.com/openshift/coredns/[]|https://github.com/openshift/coredns/pull/52] with every OpenShift 4.y release, so that OpenShift benefits from upstream performance and security fixes, and so that we avoid large version-number jumps when an urgently needed change necessitates bumping CoreDNS to the latest upstream release. This bump should happen as early as possible in the OpenShift release cycle, so as to maximize soak time.\n",
      "\n",
      "¬†\n",
      "\n",
      "For OpenShift 4.9, this means bumping from CoreDNS 1.8.1 to 1.8.3, or possibly a later release should one ship before we do the bump.\n",
      "\n",
      "¬†\n",
      "\n",
      "Note that CoreDNS upstream does not maintain release branches‚Äîthat is, once CoreDNS is released, there will be no further 1.8.z releases‚Äîso we may be better off updating to 1.9 as soon as it is released, rather than staying on the 1.8 series which would then be unmaintained.\n",
      "\n",
      "¬†\n",
      "\n",
      "We may consider bumping CoreDNS again during the OpenShift 4.9 release cycle if upstream ships additional releases during the 4.9 development cycle. However, we will need to weigh the risks and available remaining soak time in the release schedule before doing so, should that contingency arise.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 357,\n",
      "  'index_start': 336,\n",
      "  'text': 'OpenShift 4.y release'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 479,\n",
      "  'index_start': 459,\n",
      "  'text': 'version-number jumps'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 647,\n",
      "  'index_start': 624,\n",
      "  'text': 'OpenShift release cycle'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1176,\n",
      "  'index_start': 1149,\n",
      "  'text': 'OpenShift 4.9 release cycle'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13431493 --------------------------------------------------\n",
      "\n",
      "As a new mac owner, i want to able to run podman machine on my apple silicon mac.¬† Once the fcos for aarch64 images become available, we should use the same logic as Intel to download the correct image.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 80,\n",
      "  'index_start': 63,\n",
      "  'text': 'apple silicon mac'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13263774 --------------------------------------------------\n",
      "\n",
      "USER STORY:\n",
      "As a CEE associate[s]/orginaization, I want to know about new features and how these new features impacts customers, as well as how they (customers) interact with it. We also want to know about the features development, how it was tested (or how we plan to test it), and how it is documented (or how we plan to document it). \n",
      "\n",
      "DESCRIPTION:\n",
      "CEE needs a new feature technical enablment overview (slide deck, pre-created: https://docs.google.com/presentation/d/1CKyP_oBToK9D1q60l3DnSo-rtKZTWeBZhm8d9io7yc8/edit?usp=drivesdk), \n",
      "that outlines, what \"Single Node\" (feature or set of features) are/is being deliverd. \n",
      "CEE endevors to understand how we are testing, and how we are documenting the subject, so please coordinate with the proper functional teams to complete this effort (it is part of the definition of DONE). \n",
      "\n",
      "Required:\n",
      "- slide deck - https://docs.google.com/presentation/d/1CKyP_oBToK9D1q60l3DnSo-rtKZTWeBZhm8d9io7yc8/edit?usp=drivesdk\n",
      "    - This is pre-created for you, you simply need to edit the supplied deck.\n",
      "\n",
      "Important Considerations:\n",
      "- Additional details on how to troubeshoot or debug this component/feature, should be a focal point. \n",
      "   - *Note*: all efforts you put in here, ultimatly go's to reducing the LOE needed to traiage and work bugs, \n",
      "       as CEE will be more prepared, and better equiped when filing issues, or addressing customer cases. \n",
      "\n",
      "More Details: \n",
      "For more information please see: https://docs.google.com/document/d/1-EjXidVHA4hB2XezuObfb7LY9xhLEIHijxIsj_ocQfw/edit#\n",
      "\n",
      "ACCEPTANCE CRITERIA:\n",
      "- All functions have the slide deck created by feature complete, so that CEE have them provided for review (to test/explore with nightly builds). \n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 956,\n",
      "  'index_start': 842,\n",
      "  'text': 'slide deck - '\n",
      "          'https://docs.google.com/presentation/d/1CKyP_oBToK9D1q60l3DnSo-rtKZTWeBZhm8d9io7yc8/edit?usp=drivesdk'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13197103 --------------------------------------------------\n",
      "\n",
      "h2. User Story\n",
      "\n",
      "As an OpenShift cluster admin\n",
      "I want to be alerted if something cleared the LastImageTriggeredID in a BuildConfig\n",
      "So that I am aware that cluster users are relying on deprecated behavior.\n",
      "\n",
      "h2. Acceptance Criteria\n",
      "\n",
      "* Cluster administrators can use system information to identify if a build was triggered by having the LastImagechangeTriggerID cleared\n",
      "* Deprecation notice regarding this behavior is visible in the web console.\n",
      "\n",
      "h2. Launch Checklist\n",
      "\n",
      "(?) Dependencies identified\n",
      "(?) Blockers noted and expected delivery timelines set\n",
      "(?) Design is implementable\n",
      "(?) Acceptance criteria agreed upon\n",
      "(?) Story estimated\n",
      "\n",
      "h2. Notes\n",
      "\n",
      "After review with the monitoring team, we agreed to use Warning events rather than alerts. To properly create an alert for this behavior, we would have needed to introduce a metric with unbound cardinality. Such metrics can lead to performance degradation in the cluster monitoring system.\n",
      "\n",
      "When the {{LastImageChangeTriggerID}} is cleared in a BuildConfig object, a warning event is created which should appear in relevant contexts within the Web Console.\n",
      "\n",
      "----\n",
      "\n",
      "h2. Guiding Questions\n",
      "\n",
      "h3. User Story\n",
      "\n",
      "* Is this intended for an administrator, application developer, or other type of OpenShift user?\n",
      "* What experience level is this intended for? New, experienced, etc.?\n",
      "* Why is this story important? What problems does this solve? What benefit(s) will the customer experience?\n",
      "* Is this part of a larger epic or initiative? If so, ensure that the story is linked to the appropriate epic and/or initiative.\n",
      "\n",
      "h3. Acceptance Criteria\n",
      "\n",
      "* How should a customer use and/or configure the feature?\n",
      "* Are there any prerequisites for using/enabling the feature?\n",
      "\n",
      "h3. Notes\n",
      "\n",
      "* Is this a new feature, or an enhancement of an existing feature? If the latter, list the feature and docs reference.\n",
      "* Are there any new terms, abbreviations, or commands introduced with this story? Ex: a new command line argument, a new custom resource.\n",
      "* Are there any recommended best practices when using this feature?\n",
      "* On feature completion, are there any known issues that customers should be aware of?\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 45,\n",
      "  'index_start': 22,\n",
      "  'text': 'OpenShift cluster admin'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 481,\n",
      "  'index_start': 447,\n",
      "  'text': 'Launch Checklist\\n\\n(?) Dependencies'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 932,\n",
      "  'index_start': 907,\n",
      "  'text': 'cluster monitoring system'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1941,\n",
      "  'index_start': 1920,\n",
      "  'text': 'command line argument'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12961673 --------------------------------------------------\n",
      "\n",
      "As a deployer, I want a fully managed blob storage to be included with my Quay installation in order to reduce the number of individual components I need to configure.\n",
      "\n",
      "Acceptance criteria:\n",
      "- needs to support a Red Hat-supported Operator providing a managed blob storage service\n",
      "- can additionally support 3rd-party supported Operators providing a managed blob storage service\n",
      "- may (not solely) support community-sourced Operators providing a managed blob storage service\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 278,\n",
      "  'index_start': 258,\n",
      "  'text': 'blob storage service'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 376,\n",
      "  'index_start': 356,\n",
      "  'text': 'blob storage service'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 472,\n",
      "  'index_start': 452,\n",
      "  'text': 'blob storage service'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12585565 --------------------------------------------------\n",
      "\n",
      "As a user with monitor privileges I can bulk modify a list of alert definitions that I have created.\n",
      "Verify the user can select a subset of the alert definitions\n",
      "Verify the user can modify common properties of the select subset of alert definitions\n",
      "---Examples\n",
      "Notification Email Address\n",
      "Notification Text Address\n",
      "Alert Definition Severity\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 339,\n",
      "  'index_start': 261,\n",
      "  'text': 'Notification Email Address\\n'\n",
      "          'Notification Text Address\\n'\n",
      "          'Alert Definition Severity'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13425231 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a user, I want to export my applications in my namespace.\n",
      "h3. Acceptance Criteria\n",
      " * button in topology header to start the export\n",
      " * the button will create the Export CR\n",
      " ** if the CR already exists, delete the previous one if it is completed\n",
      " ** if the CR already exists and the export process is still in progress, show a modal message\n",
      " * Once the export completes, the Export CR status will update with a URL which the user can access to download the archive\n",
      " * When the URL appears in the status, the UI will display a toast notification which includes the URL link for the user to download the archive. The toast should not auto dismiss. The toast can only be dismissed by the user.\n",
      " * If the user initiates an export and then leaves the page but then returns, the UI should still notify the user that the export has completed or when the export completes\n",
      "\n",
      "h3. Additional Details:\n",
      " * Only notify the originator of the export when the export completes\n",
      " * store the export details in the user settings and clear it after the notification is given\n",
      " ** on page load, check the user settings for this item and start watching the export resource at that time\n",
      " *** keep enough information present to know that the export resource is the same one that was originally created and not one of the same name which was deleted and re-created\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 111,\n",
      "  'index_start': 82,\n",
      "  'text': 'Acceptance Criteria\\n * button'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 409,\n",
      "  'index_start': 393,\n",
      "  'text': 'Export CR status'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12615650 --------------------------------------------------\n",
      "\n",
      "As a Content Manager I want to be able to add images to Articles.\n",
      "\n",
      "h2. Definition of Done\n",
      "* An article can be created with an embedded image\n",
      "* The image can be added to any part of the Article's body\n",
      "* An image can be added/removed from an existing Article.\n",
      "\n",
      "[~dcoughlin1]'s thoughts on this:\n",
      "{quote}\n",
      "Drupal's add article feature supports an editor that allows four different input types. The default is \"basic html\" as provided by Drupal. The basic functionality isn't enough to include image tags and my guess is that it has other limitations. Can we switch the default to \"Full HTML\" or \"Red Hat approved html\" instead?\n",
      "{quote}\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 329,\n",
      "  'index_start': 310,\n",
      "  'text': 'add article feature'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13314669 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a developer, I should use PF components whenever available.\n",
      "h3. Acceptance Criteria\n",
      " # Should update Kebab to use PF menu and related components.\n",
      " # Should use custom Popper implementation for positioning of the menus.\n",
      " # Should create a generic MenuItems components that can be used with any type of toggle.\n",
      "\n",
      "h3. Additional Details:\n",
      " # [https://www.patternfly.org/v4/components/menu]¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 106,\n",
      "  'index_start': 84,\n",
      "  'text': 'Acceptance Criteria\\n #'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 208,\n",
      "  'index_start': 180,\n",
      "  'text': 'custom Popper implementation'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13253774 --------------------------------------------------\n",
      "\n",
      "h2. Spike User Story\n",
      "\n",
      "As a developer\n",
      "I want to tune the output image of Shipwright builds\n",
      "So that I can customize labels added to images\n",
      "\n",
      "h2. Acceptance Criteria\n",
      "\n",
      "Upstream enhancement proposal which addresses the following:\n",
      "\n",
      "* Output labels can be defined in a Build and are added to the output image. Must work for Buildah, Kaniko, and Buildpacks\n",
      "\n",
      "h2. Notes\n",
      "\n",
      "Dynamic output image tags will be researched in a follow-up enhancement proposal.\n",
      "This EP will lay the groundwork for manipulating build output images.\n",
      "\n",
      "Core question - to what extent should an image building tool know how to push and manipulate images?\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 20,\n",
      "  'index_start': 4,\n",
      "  'text': 'Spike User Story'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 192,\n",
      "  'index_start': 142,\n",
      "  'text': 'Acceptance Criteria\\n\\nUpstream enhancement proposal'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 385,\n",
      "  'index_start': 353,\n",
      "  'text': 'Notes\\n\\nDynamic output image tags'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 510,\n",
      "  'index_start': 491,\n",
      "  'text': 'build output images'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 573,\n",
      "  'index_start': 554,\n",
      "  'text': 'image building tool'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13004318 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a user, I want the console to save my settings such that upon returning to the console on another device or browser, my settings are restored.\n",
      "\n",
      "The purpose of this story is to make saving and restoring user settings easy for the developer.\n",
      "h3. Acceptance Criteria\n",
      " # utility that can fetch user settings from the backend\n",
      " # utility that can save user settings to the backend\n",
      " # api to retrieve and store individual settings\n",
      " # user settings should be loaded only once.\n",
      " ** Further discussion on the need to \"watch\" the user settings may change this AC\n",
      " # Multiple requests to save settings in a short timeframe should be batched\n",
      "\n",
      "h3. Additional Details:\n",
      " * Implementation _*ideas*_ for hook:\n",
      "\n",
      "Hook to get and set an individual user setting. Abstraction over browser storage and persisted storage.\n",
      "\n",
      "¬†\n",
      "{code:java}\n",
      "useUserSettings = (key: string, defaultValue: string): [string, (value: string) => void];{code}\n",
      "¬†\n",
      "\n",
      "¬†\n",
      "{code:java}\n",
      "const [value, setValue] = useUserSettings(key, defaultValue);{code}\n",
      "¬†\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 294,\n",
      "  'index_start': 264,\n",
      "  'text': 'Acceptance Criteria\\n # utility'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 592,\n",
      "  'index_start': 569,\n",
      "  'text': 'AC\\n # Multiple requests'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12949344 --------------------------------------------------\n",
      "\n",
      "*Goal:* A solution that enables the customer to use similar tags in different providers and getting them identified as the same tag, independently on the namespace, and also take into account small differences in tagging format between providers.\n",
      "\n",
      "*User story: *As a admin I want the ability to set up maping between different providers. For instance, matching tags that belong to the same format with a regexp, or matching tags in capital/small letters, independently of the namespace\n",
      "\n",
      " *Scope of doc:* Changes Managing cost data using tagging\n",
      "=================================================\n",
      "\n",
      "*Title of doc: *\n",
      "\n",
      "*Content Type: *\n",
      "\n",
      "*Published Output:*\n",
      "\n",
      "*Engineering Aha: * [https://redhatmbu.aha.io/epics/COST-E-101|https://redhatmbu.aha.io/epics/COST-E-101] \n",
      "\n",
      "*Docs preview/working doc:*\n",
      "\n",
      "* SMEs: *\n",
      "\n",
      "*Technical reviewer(s): *\n",
      "\n",
      "*Support reviewer:*\n",
      "\n",
      "*Deadline: *\n",
      "\n",
      "++++++++++++++++++*Additional information:*++++++++++++++++\n",
      "\n",
      "*Problem:* \n",
      "Different providers (AWS and OCP), have different definition for tags. We differentiate them by using a full namespace that isolates one vendor from other. In addition, many of those providers have restrictions on the format and characters that are used for tags (i.e. being able to use punctuation marks, differentiating between capital and small letters or not). In order to identify things that belongs together between providers, we need a way to identify tags that are equal even if the namespace is different and cope with the fact of matching tags with different restrictions.\n",
      "\n",
      "In other cases, different groups can tag assets with their own strategy before being merged into an account or they can basically use different tags for the same purposes. In those cases, it is possible that assets are being identified in different sources like \"R&D\", \"Research\", \"research and development\", thus being identified as being different even if they basically belong to the same concept.\n",
      "\n",
      "*Why is this important?*\n",
      "Tagging is a basic capability necessary to provide additional information to the customer. Identifying the project, service or group that is using the resources in different ways is key to group costs together and provide meaningful information to the customer\n",
      "\n",
      "*Examples*\n",
      "* Example 1. The user tags the resources in OpenShift for its project \"Project X\", and he does the same with resources like an RDS that he sets up in AWS, using a tag that is equivalent. When we report on the project, we can identify all resources.\n",
      "\n",
      "* Example 2. The user tags one account research assets as \"R&D\", and another as \"research\". Cost Management identifies both as the same tag\n",
      "\n",
      "*New tagging presentation:*\n",
      "\n",
      "[https://docs.google.com/presentation/d/1K4wj67eu7CUGO81WtVo47G62c8MvPhIwByljiQ1mqHk/edit?usp=sharing|https://docs.google.com/presentation/d/1K4wj67eu7CUGO81WtVo47G62c8MvPhIwByljiQ1mqHk/edit?usp=sharing]\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 530,\n",
      "  'index_start': 512,\n",
      "  'text': 'Managing cost data'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2525,\n",
      "  'index_start': 2502,\n",
      "  'text': 'account research assets'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13234617 --------------------------------------------------\n",
      "\n",
      "_As an Analytics user, I want to learn to use the newly expanded job explorer page to filter my view on the job explorer page and explore each job in further detail._\n",
      "\n",
      "Task: Update the Analytics documentation to reflect the UI changes to the job explorer page.\n",
      "\n",
      "Relevant PRs for more info:\n",
      " * [https://github.com/RedHatInsights/tower-analytics-backend/issues/329]\n",
      " * [https://github.com/RedHatInsights/tower-analytics-frontend/issues/127]\n",
      "\n",
      "¬†\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 82,\n",
      "  'index_start': 65,\n",
      "  'text': 'job explorer page'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 125,\n",
      "  'index_start': 108,\n",
      "  'text': 'job explorer page'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 259,\n",
      "  'index_start': 242,\n",
      "  'text': 'job explorer page'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13411000 --------------------------------------------------\n",
      "\n",
      "**USER STORY:**\n",
      "\n",
      "As a SPLAT engineer, I want to see console output of all machines before a cluster uninstall is initiated so that I can better troubleshoot issues causing nodes to not bootstrap.\n",
      "\n",
      "**DESCRIPTION:**\n",
      "\n",
      "Its possible to have a bootstrapping issue that requires console output of the failing VM. Console output is not currently captured for IPI jobs in the deprovision gather steps. A specific type of issue to troubleshoot is [https://bugzilla.redhat.com/show_bug.cgi?id=1980544] which may have happened today in [https://prow.ci.openshift.org/view/gs/origin-ci-test/logs/periodic-ci-openshift-release-master-nightly-4.8-e2e-vsphere/1414525102975881216] but we cannot confirm since we cannot see the master-2 serial console.\n",
      "\n",
      "**Required:**\n",
      "\n",
      "Console output from each running VM in a job and stored in the Prow job artifacts.\n",
      "\n",
      "**ACCEPTANCE CRITERIA:**\n",
      " * One console screenshot per VM\n",
      " * PNG format or some universally usable image type\n",
      "\n",
      "OR\n",
      " * Serial console output saved in a text file\n",
      "\n",
      "AND\n",
      " * Named appropriately (VM-name.png, VM-name.txt...)\n",
      "\n",
      "**ENGINEERING DETAILS:**\n",
      "\n",
      "Can use govc for screenshot:\n",
      "{code:java}\n",
      "$ govc vm.console -h{code}\n",
      "Review this for serial console access:\n",
      "\n",
      "[https://github.com/jcpowermac/vmware-vspc-container]\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 766,\n",
      "  'index_start': 739,\n",
      "  'text': 'Required:**\\n\\nConsole output'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 907,\n",
      "  'index_start': 891,\n",
      "  'text': 'VM\\n * PNG format'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12612844 --------------------------------------------------\n",
      "\n",
      "The Red Hat developer team has a new build server (stumpjumper) that is to be used for builds moving forwards.\n",
      "\n",
      "As a Red Hat Developer team member, I want to migrate the current build jobs to the new Jenkins slave.\n",
      "\n",
      "Acceptance Criteria:\n",
      "\n",
      "* All current build jobs are migrated to the new Jenkins slave.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 26,\n",
      "  'index_start': 4,\n",
      "  'text': 'Red Hat developer team'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 62,\n",
      "  'index_start': 37,\n",
      "  'text': 'build server (stumpjumper'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 146,\n",
      "  'index_start': 117,\n",
      "  'text': 'Red Hat Developer team member'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12973629 --------------------------------------------------\n",
      "\n",
      "As a deployer, I want all necessary Quay config secret keys to be generated for me if I do not provide them in order to have less config fields to worry about.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 59,\n",
      "  'index_start': 36,\n",
      "  'text': 'Quay config secret keys'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13181746 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a developer , i would like to try out new changes supported for camel Connectors in the form of Kamelets for source type\n",
      "h3. Acceptance Criteria\n",
      " # Should be able to list kamletes as described in proposal doc with icons/description/Json Schema (need follow up with Cakmel/Syndesis team)\n",
      " # Make sure supported one as planned has correct¬†icons/description/Json Schema for kamelets\n",
      " # Should be able to create/verify Kamelet Binding for all supported connectors and verify if created KB has required annotations¬†\n",
      " # List the resources created for above and see where they fit in topology\n",
      " # Discuss with PM/UX and decide on what should be shown to user in topology and if any changes needed for sidebar panel\n",
      "\n",
      "h3. Additional Details:\n",
      "\n",
      "Proposal Doc [https://docs.google.com/document/d/1vwnJh8kZTPreAQ4fWT-SJg3JiFKE215ovFZ-pCf0pkA/edit#heading=h.rjrkctmedt3u]¬†\n",
      "\n",
      "channel : [https://chat.google.com/u/1/room/AAAAW4YJyws]¬†\n",
      "\n",
      "https://github.com/openshift-integration/kamelet-catalog\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 143,\n",
      "  'index_start': 129,\n",
      "  'text': 'source type\\nh3'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 167,\n",
      "  'index_start': 145,\n",
      "  'text': 'Acceptance Criteria\\n #'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 305,\n",
      "  'index_start': 285,\n",
      "  'text': 'Cakmel/Syndesis team'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 885,\n",
      "  'index_start': 753,\n",
      "  'text': 'Proposal Doc '\n",
      "          '[https://docs.google.com/document/d/1vwnJh8kZTPreAQ4fWT-SJg3JiFKE215ovFZ-pCf0pkA/edit#heading=h.rjrkctmedt3u]\\xa0\\n'\n",
      "          '\\n'\n",
      "          'channel'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12619373 --------------------------------------------------\n",
      "\n",
      "As a developers.redhat.com team member, I need to be able to easily run Drupal in a variety of environments so that content editing is made easier.\n",
      "\n",
      "*Definition of Done*\n",
      "\n",
      "* Drupal can be run in each environment supported by the v1 release\n",
      "* Environment credentials that are sensitive e.g. database passwords are not kept in a publicly accessible location e.g. GitHub\n",
      "* Endpoints of other services within the environment that are depended on e.g. searchikso, can be easily configured as properties and are not hardcoded\n",
      "* In Pull Request and Development environments, supported services e.g. searchisko are started to ensure that Drupal works as expected\n",
      "\n",
      "h2. *Possible Implementation notes:*\n",
      "\n",
      "h3. *Environment file layout:*\n",
      "\n",
      "This is my ([~rblake@redhat.com]) proposal on how we implement this:\n",
      "\n",
      "We should support the following environments:\n",
      "\n",
      "# Dev (Awestruct environment)\n",
      "# Dev (Drupal environment)\n",
      "# Pull Request (Awestruct environment)\n",
      "# Pull Request (Drupal environment)\n",
      "# Staging (Drupal environment)\n",
      "# Production (Drupal environment)\n",
      "\n",
      "*Note*: We do not need to support the current staging and production environments as they do not run Docker.\n",
      "\n",
      "For each of the above environments, create a folder within the ~_docker directory of the developers.redhat.com repository e.g:\n",
      "\n",
      "# _docker/env/production_drupal\n",
      "# _docker/env/staging_drupal\n",
      "# _docker/env/pull_request_awestruct\n",
      "# _docker/env/pull_request_drupal\n",
      "# _docker/env/dev_drupal\n",
      "# _docker/env/dev_awestruct\n",
      "\n",
      " Inside this folder, I would expect there to be at least two files:\n",
      "\n",
      "# docker-compose.yml\n",
      "# env.properties\n",
      "\n",
      "The docker-compose.yml is the docker-compose definition for that environment. As we are giving each environment its own file, there will be no need to template the files. We can be totally explicit in each environment about how ports should be mapped to the host running Docker etc. In addition the docker-compose.yml file should only contain things that will actually run in that environment e.g. there should be no mention of Selenium Hub in the production profile.\n",
      "\n",
      "env.properties is a file that will contain all of the endpoints to be used in that environment. We need this so that our Drupal code can refer to Drupal variables and not-hardcoded values, so additionally this may only be needed within the Drupal based environments (implementor to work this out). This file will need to be read by Drupal at runtime and the contents set as Drupal variables.\n",
      "\n",
      "Finally, there will need to be a modification to the control.rb script to take an  'environment' parameter e.g:\n",
      "\n",
      "\n",
      "{code:shell}\n",
      "./control.rb --env dev_awstruct --run-the-stack\n",
      "{code}\n",
      "\n",
      "Based upon this environment parameter, control.rb should load the correct docker-compose.yml file for that environment, or fail-fast if an \"unknown\" environment has been specified.\n",
      " \n",
      "If this environment value is not set, then it should default to the current Pull Request Awestruct environment so that the current Jenkins build job works without any changes.\n",
      "\n",
      "*All modifications to control.rb should be fully unit-tested.*\n",
      "\n",
      "h3. *Drupal work*\n",
      "\n",
      "There are currently hard-coded values in some of the RHD Drupal files. These all need to be migrated to be application properties which are held externally in property files and then imported as Drupal variables at run-time.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 38,\n",
      "  'index_start': 5,\n",
      "  'text': 'developers.redhat.com team member'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 721,\n",
      "  'index_start': 698,\n",
      "  'text': 'Environment file layout'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 952,\n",
      "  'index_start': 938,\n",
      "  'text': '# Pull Request'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1340,\n",
      "  'index_start': 1313,\n",
      "  'text': 'docker/env/staging_drupal\\n#'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1411,\n",
      "  'index_start': 1379,\n",
      "  'text': 'docker/env/pull_request_drupal\\n#'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2544,\n",
      "  'index_start': 2518,\n",
      "  'text': \"environment' parameter e.g\"},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 2910,\n",
      "  'index_start': 2876,\n",
      "  'text': 'Pull Request Awestruct environment'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 3129,\n",
      "  'index_start': 3113,\n",
      "  'text': 'RHD Drupal files'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13394662 --------------------------------------------------\n",
      "\n",
      "h1. Story\n",
      "\n",
      "As a developer cluster logging,\n",
      " I want log messages to be discarded after an explicit time by default\n",
      " so messages are not retried for ever and block newer log messages\n",
      "h1. Acceptance Criteria\n",
      " * Messages are not retried for ever\n",
      " * Messages are only tried for 60m by default\n",
      " * Retry timeout is can be tuned\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 41,\n",
      "  'index_start': 16,\n",
      "  'text': 'developer cluster logging'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 183,\n",
      "  'index_start': 168,\n",
      "  'text': 'log messages\\nh1'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14297867 --------------------------------------------------\n",
      "\n",
      "h2. User Story:\n",
      "\n",
      "As an installer team member, I want to be able to:\n",
      " * provide knowledge, guidance, and PR reviews to the IBM Cloud work.\n",
      "\n",
      "so that I can achieve\n",
      " * enabling the addition of the IBM Cloud provider to the installer.\n",
      "\n",
      "h2. Acceptance Criteria:\n",
      "\n",
      "Description of criteria:\n",
      " * I am responding to questions regarding the installer (including CI) in a timely manner.\n",
      " * I am reviewing and providing feedback to installer and release PRs in a timely manner.\n",
      " * I am approving installer PRs in a timely manner.\n",
      "\n",
      "h2. (optional) Out of Scope:\n",
      "\n",
      "We are not writing any code as part of this story.\n",
      "h2. Engineering Details:\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 44,\n",
      "  'index_start': 23,\n",
      "  'text': 'installer team member'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 136,\n",
      "  'index_start': 122,\n",
      "  'text': 'IBM Cloud work'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 211,\n",
      "  'index_start': 193,\n",
      "  'text': 'IBM Cloud provider'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13331245 --------------------------------------------------\n",
      "\n",
      "As a platform admin I would like to easily install PAC. Ideally from an operator with minimum interactions but good enough for MVP from a single \n",
      "\n",
      "\n",
      "{code:java}\n",
      "kubectl install -f /path/to/pipeline-as-code/release.yaml\n",
      "{code}\n",
      "\n",
      "\n",
      "h4. GitHub apps instalation\n",
      "\n",
      "* Admin needs to create a GitHUB application to connect to PAC. A [Github app sitemap|https://docs.github.com/en/developers/apps/creating-a-github-app-from-a-manifest#:~:text=App%20Manifest%20flow-,About%20GitHub%20App%20Manifests,to%20automatically%20register%20the%20app] can be provided to user to make it as easy as possible\n",
      "* User would need to create a secret for private key and a configmap for settings : \n",
      "\n",
      "https://github.com/tektoncd/catalog/blob/4353aa8d50f2ad74bd769860d6f3deafc89d1dfe/task/github-app-token/0.1/README.md#L20-L22\n",
      "\n",
      "h4. Details\n",
      "\n",
      "As decided in 21/04/21 meeting on how to install we will install eventlistenner/triggers template in a dedicated namespace :\n",
      "\n",
      "\n",
      "{noformat}\n",
      "openshift-pipelines-ascode\n",
      "{noformat}\n",
      "\n",
      "\n",
      "* The PAC PipelineRuns will be started in that namespace.\n",
      "* The PAC PipelineRuns should be labelled easily by Repository/Owner/Event type for easy filtering in case user needs to look at them.\n",
      "* User pipeline event will be added in the user namespace.\n",
      "* A pruning cron should be added enabled with sane default.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 908,\n",
      "  'index_start': 876,\n",
      "  'text': 'eventlistenner/triggers template'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 985,\n",
      "  'index_start': 939,\n",
      "  'text': 'noformat}\\nopenshift-pipelines-ascode\\n{noformat'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1126,\n",
      "  'index_start': 1099,\n",
      "  'text': 'Repository/Owner/Event type'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1203,\n",
      "  'index_start': 1184,\n",
      "  'text': 'User pipeline event'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13335783 --------------------------------------------------\n",
      "\n",
      "*As a user of kam,* I want to know how I can safely store the secrets that are generated by kam in my Git repository. kam should tell me the steps required after using bootstrap.\n",
      "\n",
      "*Acceptance criteria:*\n",
      " * Some general documentation exist on how to convert the secrets created by kam into a SealedSecret resource so that they can be stored in Git and consumed by the cluster\n",
      " * kam bootstrap command gives users hints on what to do with the secrets\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 399,\n",
      "  'index_start': 378,\n",
      "  'text': 'kam bootstrap command'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12884129 --------------------------------------------------\n",
      "\n",
      "As an App SRE, I require that database schema migrations be coordinated through the user of ManagedDatabase and DatabaseMigration CRDs such that I can revision the schema in lockstep with the application servers.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 56,\n",
      "  'index_start': 30,\n",
      "  'text': 'database schema migrations'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13174152 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a user, i would like to view¬†details page for Channel of kind Channel/InMemoryChannel/KafkaChannel\n",
      "h3. Acceptance Criteria\n",
      " # user should be shown with details page for Channels\n",
      " # user should be shown with base detail layout i.e Details/yaml tab and conditions\n",
      " # user should be able to see list of Subscriptions, as a tab view and which would take user to Subscription details view on Click¬†\n",
      "\n",
      "h3. Additional Details:\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 121,\n",
      "  'index_start': 82,\n",
      "  'text': 'Channel/InMemoryChannel/KafkaChannel\\nh3'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 150,\n",
      "  'index_start': 123,\n",
      "  'text': 'Acceptance Criteria\\n # user'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 205,\n",
      "  'index_start': 189,\n",
      "  'text': 'Channels\\n # user'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 289,\n",
      "  'index_start': 227,\n",
      "  'text': 'base detail layout i.e Details/yaml tab and conditions\\n # user'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14333578 --------------------------------------------------\n",
      "\n",
      "**USER STORY:**\n",
      "\n",
      "As a m-dcn-c developer, I want create a cluster profile¬†so that I can enable e2e CI tests.\n",
      "\n",
      "**DESCRIPTION:**\n",
      "\n",
      "The intention of this story is to create a new cluster profile.¬† This cluster profile will allow a new lease pool to be created which will specifically target a vCenter which has necessary clusters and datastores to properly test [this enhancement|https://github.com/openshift/enhancements/pull/918].\n",
      "\n",
      "**Required:**\n",
      " * Merged pull request in ci-tools which adds new cluster-profile¬†\n",
      " * Merged pull request in release which creates a new lease pool in boskos and enables 4 leases\n",
      "\n",
      "**Nice to have:**\n",
      "\n",
      "**ACCEPTANCE CRITERIA:**\n",
      "\n",
      "For this story, fulfilling the requirements is the acceptance criteria\n",
      "\n",
      "**ENGINEERING DETAILS:**\n",
      "\n",
      "Example pull requests from adding the `vsphere-discon` cluster profile\n",
      "\n",
      "https://github.com/openshift/ci-tools/pull/2253\n",
      "https://github.com/openshift/release/pull/21245\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 39,\n",
      "  'index_start': 22,\n",
      "  'text': 'm-dcn-c developer'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 106,\n",
      "  'index_start': 94,\n",
      "  'text': 'e2e CI tests'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 425,\n",
      "  'index_start': 363,\n",
      "  'text': 'enhancement|https://github.com/openshift/enhancements/pull/918'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 820,\n",
      "  'index_start': 797,\n",
      "  'text': 'discon` cluster profile'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13421903 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a developer, I should be using all the dynamic extensions in helm plugin.\n",
      "h3. Acceptance Criteria\n",
      " # Should migrate all static extensions being used in plugin.ts to use dynamic extensions in console-extensions.json\n",
      "\n",
      "h3. Additional Details:\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 120,\n",
      "  'index_start': 98,\n",
      "  'text': 'Acceptance Criteria\\n #'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13246462 --------------------------------------------------\n",
      "\n",
      "h3. Overview\n",
      "\n",
      "Need an easy getting started experience for Spring Boot developers.\n",
      "\n",
      "We want to move Spring Boot developers to deploy their apps onto OpenShift, since Spring Boot developers make up a very high percentage of our prospective OpenShift users\n",
      " ¬†\n",
      "h3. Use cases:\n",
      "# As a user, I have a fat jar I want to drag and drop into OpenShift, so that it deploys\n",
      "\n",
      "h3. Acceptance criteria:\n",
      "# As a developer, I want to have a CTA available on the Add page which will allow me to Upload my jar file, choose deployment type ( Deployment, Deployment Config or Knative Service ), and optionally specify Advanced options.\n",
      "# As a developer, I want to drag my jar from my desktop to Topology (either graph or list) in OpenShift, to deploy it easily.\n",
      "## When I drop my jar, I should get a form enabling me to deploy my app\n",
      "# This upload form has a number of capabilities\n",
      "## I should be given the opportunity to choose between D/DC/KSVC\n",
      "## I should be given the opportunity to change the runtime label so I can pick my own icon\n",
      "## I should be able to provide a pipeline\n",
      "## I should be given the opportunity to set appropriate Adv Options \n",
      "# As a dev I should able to update adv options of my spring boot D, DC or Knative Service. ( Similar to Edit for Import from Git Edit flow showing the same dialog )\n",
      "# As a dev, I should be able to edit my D/DC/KSVC via _Edit %name% _ CTA to upload a new jar or change Adv Options\n",
      "# As a dev, I should be able to drag & drop a new version of my jar and drop it onto the previously created D/DC/KSVC\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 80,\n",
      "  'index_start': 58,\n",
      "  'text': 'Spring Boot developers'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 121,\n",
      "  'index_start': 99,\n",
      "  'text': 'Spring Boot developers'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 187,\n",
      "  'index_start': 165,\n",
      "  'text': 'Spring Boot developers'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 259,\n",
      "  'index_start': 238,\n",
      "  'text': 'OpenShift users\\n \\xa0\\nh3'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 923,\n",
      "  'index_start': 914,\n",
      "  'text': 'D/DC/KSVC'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1127,\n",
      "  'index_start': 1113,\n",
      "  'text': 'Adv Options \\n#'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1192,\n",
      "  'index_start': 1179,\n",
      "  'text': 'spring boot D'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1264,\n",
      "  'index_start': 1251,\n",
      "  'text': 'Git Edit flow'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1340,\n",
      "  'index_start': 1331,\n",
      "  'text': 'D/DC/KSVC'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1407,\n",
      "  'index_start': 1394,\n",
      "  'text': 'Adv Options\\n#'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1523,\n",
      "  'index_start': 1514,\n",
      "  'text': 'D/DC/KSVC'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13277087 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "Automation of editing an app in topology\n",
      "As a user,\n",
      "h3. Acceptance Criteria\n",
      " # Deletion of deployment workload\n",
      "# Deletion of knative service\n",
      "\n",
      "h3. Additional Details:\n",
      "\n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 103,\n",
      "  'index_start': 72,\n",
      "  'text': 'Acceptance Criteria\\n # Deletion'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 137,\n",
      "  'index_start': 107,\n",
      "  'text': 'deployment workload\\n# Deletion'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 160,\n",
      "  'index_start': 141,\n",
      "  'text': 'knative service\\n\\nh3'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13361457 --------------------------------------------------\n",
      "\n",
      "*Story:*¬†As a Quay administrator using the Operator I want to opt out of Operator-based TLS management, so I can rely and re-use OCP-based certificate provisioning and rotation using classic edge based {{Routes}}.\n",
      "\n",
      "*Implementation:*¬†[https://github.com/quay/enhancements/blob/main/enhancements/tls-managed-component.md]\n",
      "\n",
      "¬†\n",
      "\n",
      "*Acceptance criteria:*\n",
      " * During default deployments (without user specific configuration) TLS termination is managed by the Route\n",
      " ** Default Route for Quay must contain its spec.tls.termination field set to \"edge\".\n",
      " * Users can still provide their own key and certs (through¬†spec.configBundleSecret)\n",
      " ** When users provide their own key and cert those must be present in the created Route object.\n",
      "\n",
      "¬†\n",
      "\n",
      "*Test plan:*\n",
      "\n",
      "There is a test plan for this one, it is in [https://github.com/quay/enhancements/blob/main/enhancements/tls-managed-component.md#test-plan]\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 344,\n",
      "  'index_start': 234,\n",
      "  'text': 'https://github.com/quay/enhancements/blob/main/enhancements/tls-managed-component.md]\\n'\n",
      "          '\\n'\n",
      "          '\\xa0\\n'\n",
      "          '\\n'\n",
      "          '*Acceptance criteria'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 12937460 --------------------------------------------------\n",
      "\n",
      "As a cluster admin installing the Quay Bridge Operator, I want all necessary components to be created during install-time in order for the Operator to be ready to use immediately with no manual steps.\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 54,\n",
      "  'index_start': 34,\n",
      "  'text': 'Quay Bridge Operator'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14266434 --------------------------------------------------\n",
      "\n",
      "h3. Description\n",
      "\n",
      "As a user, I want to store my delivery pipelines in a Git repository as the source of truth and execute the pipeline on OpenShift on Git events, so that I can version and trace changes to the delivery pipelines in Git.\n",
      "h3. Use Cases\n",
      " * Developer can see the list of Git repositories that are added to the namespace for pipeline-as-code execution\n",
      " * Developer can navigate from the Console to the Git repository on the Git provider\n",
      " * For each Git repository, developer can see the details of the last pipeline execution and the commit id that triggered it with possibility to navigating to the Git commit in the Git provider\n",
      " * Developer can see the list of pipelinerun executions related to a Git repository in a chronological order and the commit id that triggered each\n",
      "\n",
      "h3. Acceptance Criteria\n",
      " # As a user, looking at the Pipelines page in the Developer Console, I should be able to see a list of (a) Git repositories that are added to the namespace for PAC execution AND (b) all pipelines in the namespace\n",
      " # As a user, I should be able to navigate to a details page of the git repo.\n",
      " ## This details page should provide access to (a) details of the git repo and (b) a list of pipeline runs.\n",
      " ## This PLR tab should show additional information than the typical PLR List view, including SHA (commit id), commit message, branch & trigger type\n",
      " # As a user, when looking at a Pipeline Run Details page, if associate with a git repo (PAC),\n",
      " ## Indicate that it's from a specific git repo rather than a PL resource\n",
      " ## Include the SHA (commit id), commit message, branch & trigger type\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 768,\n",
      "  'index_start': 759,\n",
      "  'text': 'commit id'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1127,\n",
      "  'index_start': 1108,\n",
      "  'text': '# This details page'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1296,\n",
      "  'index_start': 1283,\n",
      "  'text': 'PLR List view'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1420,\n",
      "  'index_start': 1395,\n",
      "  'text': 'Pipeline Run Details page'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14357727 --------------------------------------------------\n",
      "\n",
      "h2. *Story* (Required)\n",
      "\n",
      "_As an devops engineer,¬†¬†I will like to have test case when PR includes a file which is not part of¬† the chart , an appropriate message is output in the PR_\n",
      "h2. *Background* (Required)\n",
      "\n",
      "In our regression tests, this story will cover adding tests for when PR includes a file which is not part of the chart.\n",
      "h2. *Glossary*\n",
      "\n",
      "_NA_\n",
      "h2. *Out of scope*\n",
      "\n",
      "_NA_\n",
      "h2. *In Scope*\n",
      "\n",
      "_NA_\n",
      "h2. {*}Approach{*}(Required)\n",
      "\n",
      "_Add non chart related file in the chart._\n",
      "h2. *Dependencies*\n",
      "\n",
      "BDD Test Framework\n",
      "h2. *Edge Case*\n",
      "\n",
      "_NA_\n",
      "h2. *Acceptance Criteria*\n",
      "\n",
      "\n",
      "(?) Test case executed on each workflow change to verify behavior when PR includes a file which is not part of the chart\n",
      "h1. *INVEST Checklist*\n",
      "\n",
      "(?) Dependencies identified\n",
      "(?) Blockers noted and expected delivery timelines set\n",
      "(?) Design is implementable\n",
      "(?) Acceptance criteria agreed upon\n",
      "(?) Story estimated\n",
      "h4. Legend\n",
      "\n",
      "(?) Unknown\n",
      "(/) Verified\n",
      "(x) Unsatisfied\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 511,\n",
      "  'index_start': 475,\n",
      "  'text': 'Dependencies*\\n\\nBDD Test Framework\\nh2'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14220202 --------------------------------------------------\n",
      "\n",
      "h2. *User Story*\n",
      "\n",
      "As a user of Windows VMs¬†I want my VMs to continue working post cloud controller manager migration¬†so that my workloads continue to work, uninterrupted.\n",
      "h2. *Background*\n",
      "\n",
      "It was identified ([https://bugzilla.redhat.com/show_bug.cgi?id=1995894)]¬†that the kubelet on Windows VMs on AWS (and likely other platforms too) isn't picking up the expected hostname when starting with a CCM. This means that we need to ensure the fix in [https://github.com/openshift/machine-config-operator/pull/2694]¬†is ported to WMCO to allow the WMCO Kubelet to pickup the correct/resolvable hostname for the instance, and maintain the behaviour of the existing in-tree cloud providers in this regard\n",
      "h2. *Steps*\n",
      " * Verify which platforms (AWS, Azure, GCP, vSphere, OpenStack) are affected by this issue\n",
      " * Identify a solution, similar to the MCO fix above that maintains the hostnames during an upgrade\n",
      "\n",
      "h2. *Stakeholders*\n",
      " * Windows Containers\n",
      " * QE and Support\n",
      " * Any user of Windows Containers\n",
      "\n",
      "h2. *Definition of Done*\n",
      " * When upgraded from in-tree to out of tree on any platform, Windows Kubelets maintain their hostname and continue to operate as expected.\n",
      "\n",
      " * *Docs*\n",
      "\n",
      " * N/A\n",
      "\n",
      " * *Testing*\n",
      "\n",
      " * We will need to create clusters, add windows hosts, migrate to CCM (TechPreviewNoUpgrade feature gate), then verify the windows host health, and ensure a new windows host can join the cluster\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 116,\n",
      "  'index_start': 77,\n",
      "  'text': 'post cloud controller manager migration'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 940,\n",
      "  'index_start': 905,\n",
      "  'text': 'Stakeholders*\\n * Windows Containers'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 996,\n",
      "  'index_start': 974,\n",
      "  'text': 'Windows Containers\\n\\nh2'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1297,\n",
      "  'index_start': 1264,\n",
      "  'text': 'TechPreviewNoUpgrade feature gate'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 1335,\n",
      "  'index_start': 1316,\n",
      "  'text': 'windows host health'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14227987 --------------------------------------------------\n",
      "\n",
      "As a ARM RHCOS pipeline admin, I would like the CoreOS assembler job to work correctly. See [https://coreos.slack.com/archives/CFFJUNP6C/p1630441019052800?thread_ts=1630433224.027000&cid=CFFJUNP6C]\n",
      "\n",
      "Acceptance Criteria:\n",
      "- CoreOS assembler job working properly\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 23,\n",
      "  'index_start': 5,\n",
      "  'text': 'ARM RHCOS pipeline'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 68,\n",
      "  'index_start': 48,\n",
      "  'text': 'CoreOS assembler job'},\n",
      " {'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 242,\n",
      "  'index_start': 222,\n",
      "  'text': 'CoreOS assembler job'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13417420 --------------------------------------------------\n",
      "\n",
      "USER STORY:\n",
      "As a CEE associate[s]/orginaization, I want to know about new features and how these new features impacts customers, as well as how they (customers) interact with it. We also want to know about the features development, how it was tested (or how we plan to test it), and how it is documented (or how we plan to document it). \n",
      "\n",
      "DESCRIPTION:\n",
      "CEE needs a new feature technical enablment overview (slide deck, pre-created: https://docs.google.com/presentation/d/1qBHLKiVLThwIOlkkpHpa6fW4WCdvC4gKdQGQdCzay0o/edit?usp=drivesdk), \n",
      "that outlines, what \"Upgrade Improvments\" (feature or set of features) are/is being deliverd. \n",
      "CEE endevors to understand how we are testing, and how we are documenting the subject, so please coordinate with the proper functional teams to complete this effort (it is part of the definition of DONE). \n",
      "\n",
      "Required:\n",
      "- slide deck - https://docs.google.com/presentation/d/1qBHLKiVLThwIOlkkpHpa6fW4WCdvC4gKdQGQdCzay0o/edit?usp=drivesdk\n",
      "    - This is pre-created for you, you simply need to edit the supplied deck.\n",
      "\n",
      "Important Considerations:\n",
      "- Additional details on how to troubeshoot or debug this component/feature, should be a focal point. \n",
      "   - *Note*: all efforts you put in here, ultimatly go's to reducing the LOE needed to traiage and work bugs, \n",
      "       as CEE will be more prepared, and better equiped when filing issues, or addressing customer cases. \n",
      "\n",
      "More Details: \n",
      "For more information please see: https://docs.google.com/document/d/1-EjXidVHA4hB2XezuObfb7LY9xhLEIHijxIsj_ocQfw/edit#\n",
      "\n",
      "ACCEPTANCE CRITERIA:\n",
      "- All functions have the slide deck created by feature complete, so that CEE have them provided for review (to test/explore with nightly builds). \n",
      "\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Compound Nouns',\n",
      "  'index_end': 964,\n",
      "  'index_start': 850,\n",
      "  'text': 'slide deck - '\n",
      "          'https://docs.google.com/presentation/d/1qBHLKiVLThwIOlkkpHpa6fW4WCdvC4gKdQGQdCzay0o/edit?usp=drivesdk'}]\n"
     ]
    }
   ],
   "source": [
    "# Display the ambiguites found, one issue at a time\n",
    "display_issue_ambiguities(issues_ambs_found_compound_nouns, sample_data_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Ambiguit Detection: Nominalisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some example Nominalisations detection lexicons\n",
    "nominalisations_lexicons = {\n",
    "    'check_nominalisations': {\n",
    "        'ambiguous_nominalization'  : {\n",
    "            'title'         : 'Ambiguous Nominalization',\n",
    "            'description'   : 'A nominalization means the use of the noun form of a verb which may lead to loss of information about the nominalized action(subject, time, location).',\n",
    "            'suffixes_len2' : ['ty'] ,\n",
    "            'suffixes_len3' : ['ism', 'ion', 'ing'],\n",
    "            'suffixes_len4' : ['ment', 'ness', 'ance', 'ence'],\n",
    "            'hypernyms'     : ['event', 'process', 'act'],\n",
    "            'gerund'        : ['ing'],\n",
    "            'gerund_plural' : ['ings'],\n",
    "            'rule_exceptions' : ['activity',\n",
    "                                'application',\n",
    "                                'navigation',\n",
    "                                'notification',\n",
    "                                'feedback',\n",
    "                                'question',\n",
    "                                'reading',\n",
    "                                'clicking',\n",
    "                                'registration',\n",
    "                                'shopping',\n",
    "                                'rating',\n",
    "                                'monitoring',\n",
    "                                'following',\n",
    "                                'selection',\n",
    "                                'shopping',\n",
    "                                'transaction',\n",
    "                                'transmission',],\n",
    "            'lit_reference'  : 'NOVEL',\n",
    "            'language_construct': 'Nominalization'\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#############################################################| 100/100 [00:04<00:00, 24.96it/s]\n"
     ]
    }
   ],
   "source": [
    "#  Check for Nominalisations ambiguities using the lexicon defined above\n",
    "issues_ambs_found_nominalisations = check_for_ambiguities(issues, nominalisations_lexicons, sample_data_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------- Issue ID: 13307813 --------------------------------------------------\n",
      "\n",
      "As a mesh administrator, I want to be able to join two meshes into a federation that do not share a root certificate, so that administrative domains can be completely separate\n",
      "\n",
      "There has been some work upstream to support SPIFFE TrustBundles - they offer exactly the functionality required here, by mapping trust domains to certificate chains. We should look at cherry-picking that work, if possible.\n",
      "\n",
      "Acceptance Criteria:\n",
      " * Every mesh can define its own trust domain and cert chain\n",
      " * Proxies validate remote certificates depending on the trust domain \n",
      "\n",
      "This story covers exchange of certificate chains at Federation initialization- for continuous updates of cert chains, see MAISTRA-2238\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 376,\n",
      "  'index_start': 362,\n",
      "  'text': 'cherry - picking'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14267226 --------------------------------------------------\n",
      "\n",
      "As a pipeline, I want to follow guidelines and examples of best practices I should follow while I migrate my Catalog from SBC (sqlite based config) to FBC (file based config).\n",
      "\n",
      "¬†\n",
      "\n",
      "AC:¬†¬†\n",
      "\n",
      "An example repo exists that takes an existing Catalog, assumes some operators have moved over to FBC while the rest are still following the existing imperative workflow, and prototypes the potential workflow in building/maintaining the catalog. This example should be a descriptive set of documentation that describes how redhat pipelines should be built to handle the transition to file based configs.\n",
      "\n",
      "Workflow should include examples/discussion of the following scenarios:¬†\n",
      "\n",
      "1) Use of opm + opm migrate to build a db (that's hidden away, possibly within an internal folder) to showcase how SB operators and FB operators can co-exist within the same repo\n",
      "\n",
      "2) Discussion with examples of the steps that needs to be taken when existing SB operators are ready to move over to FB metadata.¬†¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 232,\n",
      "  'index_start': 224,\n",
      "  'text': 'existing'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 335,\n",
      "  'index_start': 327,\n",
      "  'text': 'existing'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 406,\n",
      "  'index_start': 398,\n",
      "  'text': 'building'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 430,\n",
      "  'index_start': 407,\n",
      "  'text': 'maintaining the catalog'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 922,\n",
      "  'index_start': 914,\n",
      "  'text': 'existing'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 588,\n",
      "  'index_start': 552,\n",
      "  'text': 'the transition to file based configs'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 692,\n",
      "  'index_start': 615,\n",
      "  'text': 'examples / discussion of the following scenarios 1 ) Use of opm + '\n",
      "          'opm migrate'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 908,\n",
      "  'index_start': 830,\n",
      "  'text': 'the same repo \\n'\n",
      "          '\\n'\n",
      "          ' 2 ) Discussion with examples of the steps that needs to be taken'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13372743 --------------------------------------------------\n",
      "\n",
      "As a Z engineer, I would like to conduct testing on Cluster Log Forwarding, so that the test case is thoroughly tested and bugs are reported (if discovered).\n",
      "\n",
      "Acceptance Criteria:\n",
      "* Test case fully executed\n",
      "* Bugs reported in Bugzilla\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 48,\n",
      "  'index_start': 41,\n",
      "  'text': 'testing'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 14248007 --------------------------------------------------\n",
      "\n",
      "**USER STORY:**\n",
      "\n",
      "As a user of vSphere CI, I want understand the performance of specific configurations so that I can identify emerging problems.\n",
      "\n",
      "**DESCRIPTION:**\n",
      "\n",
      "Recently, SPLAT enabled testing of various hardware version configurations and enabled testing on the IBM Cloud.¬† The intent of this card is to provide a report on the pass rates for:\n",
      " * hardware versions vs OpenShift release\n",
      " * cloud¬†vs OpenShift release\n",
      " * cloud vs hardware version\n",
      "\n",
      "**Required:**\n",
      " * Notebook should be published on the same cluster as the case report notebook\n",
      " * Report should be generated hourly\n",
      "\n",
      "¬†\n",
      "\n",
      "**Nice to have:**\n",
      "\n",
      "¬†\n",
      "\n",
      "**ACCEPTANCE CRITERIA:**\n",
      "\n",
      "‚Äì see required ‚Äì\n",
      "\n",
      "**ENGINEERING DETAILS:**\n",
      "\n",
      "notebook started by [~jcallen@redhat.com]¬†¬†https://github.com/openshift-eng/splat-sandbox/blob/main/splat-notebooks/prow/ci-job-version-trends.ipynb\n",
      "\n",
      "¬†\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 134,\n",
      "  'index_start': 126,\n",
      "  'text': 'emerging'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 102,\n",
      "  'index_start': 60,\n",
      "  'text': 'the performance of specific configurations'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 238,\n",
      "  'index_start': 188,\n",
      "  'text': 'testing of various hardware version configurations'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 275,\n",
      "  'index_start': 251,\n",
      "  'text': 'testing on the IBM Cloud'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 664,\n",
      "  'index_start': 653,\n",
      "  'text': 'ENGINEERING'}]\n",
      "\n",
      "-------------------------------------------------- Issue ID: 13197102 --------------------------------------------------\n",
      "\n",
      "h2. User Story\n",
      "\n",
      "As an OpenShift engineer\n",
      " I want to image change triggers to record their information BuildConfig status\n",
      " So that eventually changes to ImageChange triggers in the BuildConfig spec do not cause builds to launch\n",
      "\n",
      "As a developer using OpenShift to build images\n",
      " I want to trigger a build when the base image of my application changes\n",
      " So that bug fixes and security patches are applied to my application.\n",
      "h2. Acceptance Criteria\n",
      " * Trigger information for BuildConfigs is recorded in {{spec}} as well as {{status}}.\n",
      " * When a BuildConfig has an ImageChange trigger for its base image, a build is triggered when the base image is updated.\n",
      " * A deprecation note is added to the {{lastImageChangedTriggerID}} field: [https://github.com/openshift/api/blob/master/build/v1/types.go#L995]\n",
      "\n",
      "h2. Docs Impact\n",
      "\n",
      "Release note should inform customers that the {{lastImageChangeTriggeredID}} field in the BuildConfig spec is deprecated, and will be ignored in OCP 4.9. Users relying on this information should update their scripts and jobs to read the triggered image ID from BuildConfig status.¬†\n",
      "\n",
      "Please see https://issues.redhat.com/browse/RHDEVDOCS-2738¬†for more details.\n",
      "h2. Launch Checklist\n",
      "\n",
      "(?) Dependencies identified\n",
      " (?) Blockers noted and expected delivery timelines set\n",
      " (?) Design is implementable\n",
      " (?) Acceptance criteria agreed upon\n",
      " (?) Story estimated\n",
      "h2. Notes\n",
      "\n",
      "If {{lastImageTriggeredID}} is set to the empty string or nil, today this will trigger a build. By moving the data to {{status}}, we can ignore the {{lastImageTriggeredID}} field in {{spec}} in a future release. To give users proper notice, we are not going to remove the current behavior until a later release.\n",
      "\n",
      "A deprecation notice will need to be added to the release notes upon completion.\n",
      "\n",
      "Changes will be needed in the following:\n",
      "\n",
      "1. API\n",
      " 2. openshift-apiserver\n",
      " 3. openshift-controller-manager (build controller)\n",
      "\n",
      "See [https://bugzilla.redhat.com/show_bug.cgi?id=1876500].\n",
      "----\n",
      "h2. Guiding Questions\n",
      "h3. User Story\n",
      " * Is this intended for an administrator, application developer, or other type of OpenShift user?\n",
      " * What experience level is this intended for? New, experienced, etc.?\n",
      " * Why is this story important? What problems does this solve? What benefit(s) will the customer experience?\n",
      " * Is this part of a larger epic or initiative? If so, ensure that the story is linked to the appropriate epic and/or initiative.\n",
      "\n",
      "h3. Acceptance Criteria\n",
      " * How should a customer use and/or configure the feature?\n",
      " * Are there any prerequisites for using/enabling the feature?\n",
      "\n",
      "h3. Notes\n",
      " * Is this a new feature, or an enhancement of an existing feature? If the latter, list the feature and docs reference.\n",
      " * Are there any new terms, abbreviations, or commands introduced with this story? Ex: a new command line argument, a new custom resource.\n",
      " * Are there any recommended best practices when using this feature?\n",
      " * On feature completion, are there any known issues that customers should be aware of?\n",
      "\n",
      "Ambiguities Found:\n",
      "[{'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 1505,\n",
      "  'index_start': 1478,\n",
      "  'text': 'moving the data to { { status'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 1692,\n",
      "  'index_start': 1591,\n",
      "  'text': 'To give users proper notice , we are not going to remove the '\n",
      "          'current behavior until a later release . \\n'\n",
      "          '\\n'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 1989,\n",
      "  'index_start': 1968,\n",
      "  'text': 'Guiding Questions \\n h3 .'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 2554,\n",
      "  'index_start': 2528,\n",
      "  'text': 'using / enabling the feature'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 2554,\n",
      "  'index_start': 2533,\n",
      "  'text': '/ enabling the feature'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 2625,\n",
      "  'index_start': 2617,\n",
      "  'text': 'existing'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 2893,\n",
      "  'index_start': 2870,\n",
      "  'text': 'when using this feature'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 668,\n",
      "  'index_start': 657,\n",
      "  'text': 'deprecation'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 1705,\n",
      "  'index_start': 1694,\n",
      "  'text': 'deprecation'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 1771,\n",
      "  'index_start': 1761,\n",
      "  'text': 'completion'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 2633,\n",
      "  'index_start': 2596,\n",
      "  'text': 'an enhancement of an existing feature'},\n",
      " {'amb_type': 'Ambiguous Nominalization',\n",
      "  'index_end': 2919,\n",
      "  'index_start': 2901,\n",
      "  'text': 'feature completion'}]\n"
     ]
    }
   ],
   "source": [
    "# Display the ambiguites found, one issue at a time\n",
    "display_issue_ambiguities(issues_ambs_found_nominalisations, sample_data_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
